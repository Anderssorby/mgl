(in-package :mgl-gd)

(defsection @mgl-gd (:title "Gradient Descent")
  "Gradient descent is a first-order optimization algorithm. Relying
  completely on first derivatives, it does not even evaluate the
  function to be minimized. Let's see how to minimize a numerical lisp
  function with respect to some of its parameters.

  ```
  ;;; Create an object representing the sine function.
  (defparameter *diff-fn-1*
    (make-instance 'mgl-diffun:diffun
                   :fn #'sin
                   ;; We are going to optimize its only parameter.
                   :weight-indices '(0)))

  ;;; Minimize SIN. Note that there is no dataset involved because all
  ;;; parameters are being optimized.
  (minimize (make-instance 'batch-gd-optimizer :termination 1000)
            *diff-fn-1*
            :weights (make-mat 1))
  ;;; => A MAT with a single value of about -pi/2.

  ;;; Create a differentiable function for f(x,y)=(x-y)^2. X is a
  ;;; parameter whose values come from the DATASET argument passed to
  ;;; MINIMIZE. Y is a parameter to be optimized (a 'weight').
  (defparameter *diff-fn-2*
    (make-instance 'mgl-diffun:diffun
                   :fn (lambda (x y)
                         (expt (- x y) 2))
                   :parameter-indices '(0)
                   :weight-indices '(1)))

  ;;; Find the Y that minimizes the distance from the instances
  ;;; generated by the sampler.
  (minimize (make-instance 'batch-gd-optimizer :batch-size 10)
            *diff-fn-2*
            :weights (make-mat 1)
            :dataset (make-instance 'function-sampler
                                    :generator (lambda ()
                                                 (list (+ 10
                                                          (gaussian-random-1))))
                                    :max-n-samples 1000))
  ;;; => A MAT with a single value of about 10, the expected value of
  ;;; the instances in the dataset.

  ;;; The dataset can be a SEQUENCE in which case we'd better set
  ;;; TERMINATION else optimization would never finish.
  (minimize (make-instance 'batch-gd-optimizer
                           :termination 1000)
            *diff-fn-2*
            :weights (make-mat 1)
            :dataset '((0) (1) (2) (3) (4) (5)))
  ;;; => A MAT with a single value of about 2.5.
  ```

  We are going to see a number of accessors for optimizer paramaters.
  In general, it's allowed to SETF real slot accessors (as opposed to
  readers and writers) at any time during optimization and so is
  defining a method on an optimizer subclass that computes the value
  in any way. For example, to decay the learning rate on a per
  mini-batch basis:

      (defmethod learning-rate ((optimizer my-batch-gd-optimizer))
        (* (slot-value optimizer 'learning-rate)
           (expt 0.998
                 (/ (n-instances optimizer) 60000))))"
  (@mgl-gd-batch-gd-optimizer section)
  (@mgl-gd-segmented-gd-optimizer section)
  (@mgl-gd-per-weight-optimization section))

(defsection @mgl-gd-batch-gd-optimizer (:title "Batch GD Optimizer")
  (batch-gd-optimizer class)
  (n-instances (reader iterative-optimizer))
  (termination (accessor iterative-optimizer))
  (batch-size (accessor gd-optimizer))
  (learning-rate (accessor gd-optimizer))
  (momentum (accessor gd-optimizer))
  (momentum-type (reader gd-optimizer))
  (weight-decay (accessor gd-optimizer))
  (weight-penalty (accessor gd-optimizer))
  (after-update-hook (accessor gd-optimizer))
  (before-update-hook (accessor batch-gd-optimizer)))

(defsection @mgl-gd-segmented-gd-optimizer (:title "Segmented GD Optimizer")
  (segmented-gd-optimizer class)
  (n-instances (reader iterative-optimizer))
  (termination (accessor iterative-optimizer))
  (segmenter (reader segmented-gd-optimizer))
  (segments (reader segmented-gd-optimizer)))

(defsection @mgl-gd-per-weight-optimization (:title "Per-weight Optimization")
  (normalized-batch-gd-optimizer class)
  (per-weight-batch-gd-optimizer class))


;;;; Abstract gradient descent optimizer base class

(defclass base-gd-optimizer (iterative-optimizer)
  ())

(defmethod minimize* ((optimizer base-gd-optimizer) gradient-source
                      weights dataset)
  (let ((sampler (if (typep dataset 'sequence)
                     (make-random-sampler dataset)
                     dataset)))
    (while (and (not (terminate-optimization-p (n-instances optimizer)
                                               (termination optimizer)))
                (not (finishedp sampler)))
      (let ((batch (list-samples sampler (n-instances-until-update optimizer))))
        (let ((*accumulating-interesting-gradients* t))
          (accumulate-gradients* gradient-source optimizer batch 1 nil))
        (maybe-update-weights optimizer gradient-source (length batch))))))

(defgeneric n-instances-until-update (optimizer)
  (:documentation "Return the largest number of inputs guaranteed not
  to cause a change in the learner being trained."))

(defgeneric maybe-update-weights (optimizer gradient-source n-new-inputs)
  (:documentation "Update the weights being trained. N-NEW-INPUTS have
  been seen since the last time this was called."))


;;;; Common base class for BATCH-GD-OPTIMIZER,
;;;; NORMALIZED-BATCH-GD-OPTIMIZER and PER-WEIGHT-BATCH-GD-OPTIMIZER.

(defclass gd-optimizer (base-gd-optimizer)
  ((segment-set
    :reader segment-set
    :documentation "The set of segments that are to be trained. The
    ACCUMULATOR, WEIGHT-DELTAS, etc vectors are indexed by SEGMENT-SET
    indices.")
   (weight-deltas :type mat :accessor weight-deltas)
   (accumulator
    :type mat :accessor accumulator
    :documentation "An FLT vector that is accessed directly by the
    client and are used to store the sum of the computed gradient.")
   (batch-size
    :initform 1
    :initarg :batch-size :accessor batch-size
    :documentation "After having gone through BATCH-SIZE number of
    inputs, weights are updated. With BATCH-SIZE 1, one gets
    Stochastics Gradient Descent. With BATCH-SIZE equal to the number
    of instances in the dataset, one gets standard, 'batch' gradient
    descent. With BATCH-SIZE between these two extremes, one gets the
    most practical 'mini-batch' compromise.")
   (learning-rate
    :initform #.(flt 0.1) :initarg :learning-rate :accessor learning-rate
    :documentation "This is the step size along the gradient. Decrease
    it if optimization diverges, increase it if it doesn't make
    progress.")
   (momentum
    :initform #.(flt 0) :initarg :momentum :accessor momentum
    :documentation "A value in the [0, 1) interval. MOMENTUM times the
    previous weight change is added to the gradient. 0 means no
    momentum.")
   (momentum-type
    :initform :normal :initarg :momentum-type :reader momentum-type
    :type '(member :normal :nesterov)
    :documentation "One of :NORMAL and :NESTEROV. For pure
    optimization Nesterov's momentum may be better, but it also
    increases chances of overfitting.")
   (weight-decay
    :initform #.(flt 0) :initarg :weight-decay :accessor weight-decay
    :documentation "An L2 penalty. It discourages large weights, much
    like a zero mean gaussian prior. WEIGHT-DECAY * WEIGHT is added to
    the gradient to penalize large weights. It's as if the function
    whose minimum is sought had WEIGHT-DECAY*sum_i{0.5 * WEIGHT_i^2}
    added to it.")
   (weight-penalty
    :initform #.(flt 0) :initarg :weight-penalty :accessor weight-penalty
    :documentation "An L1 penalty. It encourages sparsity.
    SIGN(WEIGHT) * WEIGHT-PENALTY is added to the gradient pushing the
    weight towards negative infinity. It's as if the function whose
    minima is sought had WEIGHT-PENALTY*sum_i{abs(WEIGHT_i)} added to
    it. Putting it on feature biases consitutes a sparsity constraint
    on the features.")
   (after-update-hook
    :type 'list
    :initform () :initarg :after-update-hook :accessor after-update-hook
    :documentation "A list of functions with no arguments called after
    each weight update."))
  (:documentation "Gradient descent optimizer with momentum, weight
  decay, weight penalty. Batch size and all other parameters can be
  changed during training. One may even want to subclass this
  optimizer, define a method for BATCH-SIZE make it a function of
  N-INSTANCES.

  Depending on BATCH-SIZE, this may be stochastic or non-stochastic
  gradient descent."))

(defmethod print-object ((optimizer gd-optimizer) stream)
  (pprint-logical-block (stream ())
    (print-unreadable-object (optimizer stream :type t :identity t)
      (format stream "~S" (ignore-errors (segment-set optimizer)))))
  optimizer)

(define-descriptions (optimizer gd-optimizer)
  n-instances segment-set
  (learning-rate (learning-rate optimizer) "~,5E")
  (momentum (momentum optimizer) "~,5E")
  momentum-type
  (weight-decay (weight-decay optimizer) "~,5E")
  (weight-penalty (weight-penalty optimizer) "~,5E")
  (n-after-upate-hook (length (after-update-hook optimizer)) "~S")
  batch-size)

(defmethod initialize-optimizer* ((optimizer gd-optimizer) source weights
                                  dataset)
  (when (next-method-p)
    (call-next-method))
  (setf (slot-value optimizer 'segment-set)
        (make-instance 'segment-set :segments weights))
  (let ((n-weights (size (segment-set optimizer))))
    (setf (accumulator optimizer) (make-mat n-weights :ctype flt-ctype))
    (setf (weight-deltas optimizer) (make-mat n-weights :ctype flt-ctype))))

(defmethod segments ((optimizer gd-optimizer))
  (segments (segment-set optimizer)))

(defmethod map-gradient-sink (fn (optimizer gd-optimizer))
  (let ((segment-set (segment-set optimizer))
        (accumulator (accumulator optimizer)))
    (do-segment-set (segment start) segment-set
      (with-shape-and-displacement (accumulator (mat-size
                                                 (segment-weights segment))
                                    start)
        (funcall fn segment accumulator)))))


;;;; BATCH-GD-OPTIMIZER

(defclass batch-gd-optimizer (gd-optimizer)
  ((n-instances-in-batch
    :initform 0 :initarg :n-instances-in-batch :accessor n-instances-in-batch)
   (before-update-hook
    :type list :initform () :initarg :before-update-hook
    :accessor before-update-hook
    :documentation "A list of functions of no parameters. Each
    function is called just before a weight update takes place.
    Convenient to hang some additional gradient accumulating code
    on."))
  (:documentation "Updates all weights simultaneously after chewing
  through BATCH-SIZE inputs. PER-WEIGHT-BATCH-GD-OPTIMIZER may be a
  better choice when some weights can go unused for instance due to
  missing input values.

  Assuming that ACCUMULATOR has the sum of gradients for a mini-batch,
  the weight update looks like this:

      delta_w' += momentum * delta_w +
                  accumulator / batch_size + l2 * w + l1 * sign(w)

      w' -= learning_rate * delta_w'

  which is the same as the more traditional formulation:

      delta_w' += momentum * delta_w +
                  learning_rate * (df/dw / batch_size + l2 * w + l1 * sign(w))

      w' -= delta_w'

  but the former works better when batch size, momentum or learning
  rate change during the course of optimization. The above is with
  normal momentum, Nesterov's momentum (see MOMENTUM-TYPE) momentum is
  also available."))

(defmethod n-instances-until-update ((optimizer batch-gd-optimizer))
  ;; BATCH-SIZE may be setf'ed to a value lower than N-INSTANCES-IN-BATCH
  (max 0 (- (batch-size optimizer)
            (n-instances-in-batch optimizer))))

(defmethod maybe-update-weights ((optimizer batch-gd-optimizer)
                                 gradient-source n-new-inputs)
  (when (<= (batch-size optimizer)
            (incf (n-instances-in-batch optimizer) n-new-inputs))
    (ecase (momentum-type optimizer)
      ((:normal)
       (update-all-weights/normal optimizer))
      ((:nesterov)
       (update-all-weights/nesterov optimizer))))
  (set-n-instances optimizer gradient-source
                   (+ (n-instances optimizer) n-new-inputs)))

(defun update-all-weights/nesterov (optimizer)
  (map nil #'funcall (before-update-hook optimizer))
  (let ((accumulator (accumulator optimizer))
        (weight-deltas (weight-deltas optimizer))
        (learning-rate (learning-rate optimizer))
        (n-instances (flt (n-instances-in-batch optimizer)))
        (momentum (momentum optimizer))
        (weight-decay (weight-decay optimizer))
        (weight-penalty (weight-penalty optimizer)))
    (declare (type flt learning-rate n-instances momentum
                   weight-decay #+nil weight-penalty))
    (assert (and (<= 0 momentum) (< momentum 1)))
    (mgl-mat:scal! momentum weight-deltas)
    (mgl-mat:axpy! (/ n-instances) accumulator weight-deltas)
    (with-shape-and-displacement (weight-deltas)
      (with-shape-and-displacement (accumulator)
        (do-segment-set (segment start-in-segment-set) (segment-set optimizer)
          (let ((weights (segment-weights segment)))
            (reshape-and-displace! weight-deltas (mat-size weights)
                                   start-in-segment-set)
            (reshape-and-displace! accumulator (mat-size weights)
                                   start-in-segment-set)
            (unless (zerop weight-decay)
              (mgl-mat:axpy! weight-decay weights weight-deltas)
              (mgl-mat:scal! (- 1 (* learning-rate weight-decay)) weights))
            (mgl-mat:axpy! (- (/ learning-rate n-instances)) accumulator
                           weights)
            (mgl-mat:axpy! (- (* learning-rate momentum)) weight-deltas
                           weights)
            (unless (zerop weight-penalty)
              (add-sign! (* learning-rate weight-penalty) weights 0 accumulator)
              (axpy! 1 accumulator weights))))))
    (fill! (flt 0) accumulator)
    (setf (n-instances-in-batch optimizer) 0))
  (map nil #'funcall (after-update-hook optimizer)))

(defun update-all-weights/normal (optimizer)
  (map nil #'funcall (before-update-hook optimizer))
  (let ((accumulator (accumulator optimizer))
        (weight-deltas (weight-deltas optimizer))
        (learning-rate (learning-rate optimizer))
        (n-instances (flt (n-instances-in-batch optimizer)))
        (momentum (momentum optimizer))
        (weight-decay (weight-decay optimizer))
        (weight-penalty (weight-penalty optimizer)))
    (declare (type flt learning-rate n-instances momentum
                   weight-decay #+nil weight-penalty))
    (assert (and (<= 0 momentum) (< momentum 1)))
    (mgl-mat:scal! momentum weight-deltas)
    (mgl-mat:axpy! (flt (/ n-instances)) accumulator weight-deltas)
    (with-shape-and-displacement (weight-deltas)
      (do-segment-set (segment start-in-segment-set) (segment-set optimizer)
        (let ((weights (segment-weights segment)))
          (reshape-and-displace! weight-deltas (mat-size weights)
                                 start-in-segment-set)
          (unless (zerop weight-penalty)
            (add-sign! weight-penalty weights 1 weight-deltas))
          (unless (zerop weight-decay)
            (axpy! weight-decay weights weight-deltas))
          (mgl-mat:axpy! (- learning-rate) weight-deltas weights))))
    (fill! (flt 0) accumulator)
    (setf (n-instances-in-batch optimizer) 0))
  (map nil #'funcall (after-update-hook optimizer)))


;;;; NORMALIZED-BATCH-GD-OPTIMIZER

(defclass normalized-batch-gd-optimizer (batch-gd-optimizer)
  ((n-weight-uses-in-batch
    :accessor n-weight-uses-in-batch
    :documentation "Number of uses of the weight in its current batch."))
  (:documentation "Like BATCH-GD-OPTIMIZER but keeps count of how many
  times each weight was used in the batch and divides the accumulated
  gradient by this count instead of dividing by N-INSTANCES-IN-BATCH.
  This only makes a difference if there are missing values in the
  learner that's being trained. The main feature that distuinguishes
  this class from PER-WEIGHT-BATCH-GD-OPTIMIZER is that batches end at
  same time for all weights."))

(defun set-up-n-weight-uses (optimizer)
  (let ((n-weights (size (segment-set optimizer))))
    (setf (n-weight-uses-in-batch optimizer)
          (make-array n-weights :element-type 'index :initial-element 0))))

(defmethod initialize-optimizer* ((optimizer normalized-batch-gd-optimizer)
                                  source weights dataset)
  (call-next-method)
  (set-up-n-weight-uses optimizer))

(defmethod n-instances-until-update ((optimizer normalized-batch-gd-optimizer))
  ;; Weights are updated as in BATCH-GD-OPTIMIZER but we need to collect
  ;; weight usage statistics after each example.
  1)

(defmethod maybe-update-weights ((optimizer normalized-batch-gd-optimizer)
                                 gradient-source n-new-inputs)
  (declare (type index n-new-inputs))
  (assert (eq (momentum-type optimizer) :normal))
  (let ((accumulator (accumulator optimizer))
        (n-weight-uses-in-batch (n-weight-uses-in-batch optimizer))
        (weight-deltas (weight-deltas optimizer))
        (learning-rate (learning-rate optimizer))
        (momentum (momentum optimizer))
        (weight-decay (weight-decay optimizer))
        (weight-penalty (weight-penalty optimizer))
        (batch-size (batch-size optimizer)))
    (declare (type index-vector n-weight-uses-in-batch)
             (type flt learning-rate momentum weight-decay weight-penalty)
             (type index batch-size))
    (with-facets ((accumulator (accumulator 'array :direction :io
                                            :type flt-vector))
                  (weight-deltas (weight-deltas 'array :direction :io
                                                :type flt-vector)))
      (do-segment-set (segment start-in-segment-set) (segment-set optimizer)
        (map-segment-runs
         (lambda (start end)
           (declare (type index start end)
                    (optimize (speed 3) #.*no-array-bounds-check*))
           (do ((i (the! index (+ start-in-segment-set start))
                   (the! index (1+ i)))
                (j start (1+ j)))
               ((<= end j))
             (setf (aref n-weight-uses-in-batch i)
                   (the! index
                         (+ n-new-inputs
                            (the! index
                                  (aref n-weight-uses-in-batch i)))))))
         segment))
      (when (<= batch-size (the index (incf (n-instances-in-batch optimizer)
                                            n-new-inputs)))
        (setf (n-instances-in-batch optimizer) 0)
        (do-segment-set (segment start-in-segment-set) (segment-set optimizer)
          (let* ((weights (segment-weights segment))
                 (start (mat-displacement weights))
                 (end (the! index (+ start (the index (mat-size weights))))))
            (declare (optimize (speed 3) #.*no-array-bounds-check*)
                     (type index start end))
            (with-facets ((weights (weights 'backing-array :direction :io
                                            :type flt-vector)))
              (do ((i start-in-segment-set (the! index (1+ i)))
                   (j start (1+ j)))
                  ((<= end j))
                (let ((delta (+ (* momentum (aref weight-deltas i))
                                (* (if (zerop (aref n-weight-uses-in-batch i))
                                       #.(flt 0)
                                       (/ (flt
                                           (aref n-weight-uses-in-batch i))))
                                   (aref accumulator i))
                                (* weight-decay (aref weights j))
                                weight-penalty)))
                  (setf (aref weight-deltas i) delta)
                  (decf (aref weights j) (* learning-rate delta))
                  (setf (aref n-weight-uses-in-batch i) 0
                        (aref accumulator i) #.(flt 0)))))))
        (map nil #'funcall (after-update-hook optimizer)))))
  (set-n-instances optimizer gradient-source
                   (+ (n-instances optimizer) n-new-inputs)))


;;;; PER-WEIGHT-BATCH-GD-OPTIMIZER

(defclass per-weight-batch-gd-optimizer (gd-optimizer)
  ((n-weight-uses-in-batch
    :accessor n-weight-uses-in-batch
    :documentation "Number of uses of the weight in its current batch."))
  (:documentation "This is much like BATCH-GD-OPTIMIZER but it is more
  clever about when to update weights. Basically every weight has its
  own batch independent from the batches of others. It has desirable
  properties. One can for example put two neural networks together
  without adding any connections between them and the learning will
  produce results equivalent to the separated case. Also, adding
  inputs with only missing values does not change anything."))

(defmethod initialize-optimizer* ((optimizer per-weight-batch-gd-optimizer)
                                  source weights dataset)
  (call-next-method)
  (set-up-n-weight-uses optimizer))

(defmethod n-instances-until-update ((optimizer per-weight-batch-gd-optimizer))
  ;; Weight updates are async, don't overpromise.
  1)

(defmethod maybe-update-weights ((optimizer per-weight-batch-gd-optimizer)
                                 gradient-source n-new-inputs)
  (assert (= 1 n-new-inputs))
  (assert (eq (momentum-type optimizer) :normal))
  (let ((accumulator (accumulator optimizer))
        (n-weight-uses-in-batch (n-weight-uses-in-batch optimizer))
        (weight-deltas (weight-deltas optimizer))
        (learning-rate (learning-rate optimizer))
        (momentum (momentum optimizer))
        (weight-decay (weight-decay optimizer))
        (weight-penalty (weight-penalty optimizer))
        (batch-size (batch-size optimizer)))
    (declare (type index-vector n-weight-uses-in-batch)
             (type flt learning-rate momentum weight-decay weight-penalty)
             (type index batch-size))
    (with-facets ((accumulator (accumulator 'array :direction :io
                                            :type flt-vector))
                  (weight-deltas (weight-deltas 'array :direction :io
                                                :type flt-vector)))
      (declare (optimize (speed 3) #.*no-array-bounds-check*))
      (do-segment-set (segment start-in-segment-set) (segment-set optimizer)
        (let ((weights (segment-weights segment)))
          (with-facets ((weights (weights 'array :direction :io
                                          :type flt-vector)))
            (map-segment-runs
             (lambda (start end)
               (declare (type index start end))
               (do ((i (the! index (+ start-in-segment-set start))
                       (the! index (1+ i)))
                    (j start (1+ j)))
                   ((<= end j))
                 (when (<= batch-size
                           (setf (aref n-weight-uses-in-batch i)
                                 (1+ (the! index
                                           (aref n-weight-uses-in-batch i)))))
                   (let ((delta (+ (* momentum (aref weight-deltas i))
                                   (/ (aref accumulator i)
                                      (aref n-weight-uses-in-batch i))
                                   (* weight-decay (aref weights j))
                                   weight-penalty)))
                     (setf (aref weight-deltas i) delta)
                     (decf (aref weights j) (* learning-rate delta))
                     (setf (aref n-weight-uses-in-batch i) 0
                           (aref accumulator i) #.(flt 0))))))
             segment)))))
    (map nil #'funcall (after-update-hook optimizer)))
  (set-n-instances optimizer gradient-source
                   (+ (n-instances optimizer) n-new-inputs)))


;;;; SEGMENTED-GD-OPTIMIZER

(defclass segmented-gd-optimizer (base-gd-optimizer)
  ((segmenter
    :initarg :segmenter :reader segmenter
    :documentation "When this optimizer is initialized it loops over
    the segment of the learner with MAP-SEGMENTS. SEGMENTER is a
    function that is called with each segment and returns an optimizer
    or NIL. Several segments may be mapped to the same optimizer.
    After the segment->optimizer mappings are collected, each
    optimizer is initialized by INITIALIZE-OPTIMIZER with the list of
    segments mapped to it.")
   (optimizers :type list :reader optimizers)
   (segments :type list :reader segments))
  (:documentation "An optimizer that delegates training of segments to
  other optimizers. Useful to delegate training of different segments
  to different optimizers (capable of working with segmentables) or
  simply to not train all segments."))

(define-descriptions (optimizer segmented-gd-optimizer)
  n-instances optimizers segments)

(defmethod describe-object :after ((optimizer segmented-gd-optimizer) stream)
  (when (slot-boundp optimizer 'optimizers)
    (terpri stream)
    (dolist (optimizer (optimizers optimizer))
      (describe optimizer stream))))

(defmethod initialize-optimizer* ((optimizer segmented-gd-optimizer) source
                                  weights dataset)
  (when (next-method-p)
    (call-next-method))
  (let ((segmenter (segmenter optimizer))
        (optimizer-segments (make-hash-table :test 'eq)))
    (map nil (lambda (segment)
               (let ((optimizer (funcall segmenter segment)))
                 (when optimizer
                   (unless (gethash optimizer optimizer-segments)
                     (setf (gethash optimizer optimizer-segments)
                           nil))
                   (push segment (gethash optimizer optimizer-segments)))))
         weights)
    (let ((optimizers ()))
      (maphash (lambda (optimizer segments)
                 (initialize-optimizer* optimizer source segments dataset)
                 (push optimizer optimizers)
                 (values))
               optimizer-segments)
      (setf (slot-value optimizer 'optimizers) optimizers)
      ;; The child optimizer may not use all the segments assigned to it
      ;; so let's ask it.
      (setf (slot-value optimizer 'segments)
            (apply #'append (mapcar #'segments optimizers))))))

(defmethod maybe-update-weights ((optimizer segmented-gd-optimizer)
                                 gradient-source n-new-inputs)
  (dolist (optimizer (optimizers optimizer))
    (maybe-update-weights optimizer gradient-source n-new-inputs))
  (set-n-instances optimizer gradient-source
                   (+ (n-instances optimizer) n-new-inputs)))

(defmethod n-instances-until-update ((optimizer segmented-gd-optimizer))
  (if (optimizers optimizer)
      (loop for child-optimizer in (optimizers optimizer)
            minimizing (n-instances-until-update child-optimizer))
      nil))

(defmethod map-gradient-sink (fn (optimizer segmented-gd-optimizer))
  (dolist (optimizer (optimizers optimizer))
    (map-gradient-sink fn optimizer)))
