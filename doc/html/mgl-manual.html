<!DOCTYPE html>
<html xmlns='http://www.w3.org/1999/xhtml' xml:lang='en' lang='en'>
<head>
<title>MGL Manual</title>
<link type='text/css' href='style.css' rel='stylesheet'/>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
</head>
<body><p><a name='x-28MGL-3A-40MGL-MANUAL-20MGL-PAX-3ASECTION-29'></a></p>

<h1><span class="navigation"> <a href="#x-28-22mgl-22-20ASDF-2FSYSTEM-3ASYSTEM-29" title="(\&quot;mgl\&quot; ASDF/SYSTEM:SYSTEM)">&#8594;</a> <a href="#x-28MGL-3A-40MGL-MANUAL-20MGL-PAX-3ASECTION-29" title="(MGL:@MGL-MANUAL MGL-PAX:SECTION)">&#8634;</a></span>MGL Manual</h1>

<h2>Table of Contents</h2>

<ul>
<li><a href="#x-28-22mgl-22-20ASDF-2FSYSTEM-3ASYSTEM-29" title="(\&quot;mgl\&quot; ASDF/SYSTEM:SYSTEM)">1 mgl ASDF System Details</a></li>
<li><a href="#x-28MGL-3A-40MGL-OVERVIEW-20MGL-PAX-3ASECTION-29" title="(MGL:@MGL-OVERVIEW MGL-PAX:SECTION)">2 Overview</a>

<ul>
<li><a href="#x-28MGL-3A-40MGL-FEATURES-20MGL-PAX-3ASECTION-29" title="(MGL:@MGL-FEATURES MGL-PAX:SECTION)">2.1 Features</a></li>
<li><a href="#x-28MGL-3A-40MGL-DEPENDENCIES-20MGL-PAX-3ASECTION-29" title="(MGL:@MGL-DEPENDENCIES MGL-PAX:SECTION)">2.2 Dependencies</a></li>
<li><a href="#x-28MGL-3A-40MGL-TESTS-20MGL-PAX-3ASECTION-29" title="(MGL:@MGL-TESTS MGL-PAX:SECTION)">2.3 Tests</a></li>
<li><a href="#x-28MGL-3A-40MGL-BUNDLED-SOFTWARE-20MGL-PAX-3ASECTION-29" title="(MGL:@MGL-BUNDLED-SOFTWARE MGL-PAX:SECTION)">2.4 Bundled Software</a></li>
</ul></li>
<li><a href="#x-28MGL-3A-40MGL-BASIC-CONCEPTS-20MGL-PAX-3ASECTION-29" title="(MGL:@MGL-BASIC-CONCEPTS MGL-PAX:SECTION)">3 Basic Concepts</a></li>
<li><a href="#x-28MGL-DATASET-3A-40MGL-DATASET-20MGL-PAX-3ASECTION-29" title="(MGL-DATASET:@MGL-DATASET MGL-PAX:SECTION)">4 Dataset</a>

<ul>
<li><a href="#x-28MGL-DATASET-3A-40MGL-SAMPLER-20MGL-PAX-3ASECTION-29" title="(MGL-DATASET:@MGL-SAMPLER MGL-PAX:SECTION)">4.1 Sampler</a>

<ul>
<li><a href="#x-28MGL-DATASET-3A-40MGL-SAMPLER-FUNCTION-SAMPLER-20MGL-PAX-3ASECTION-29" title="(MGL-DATASET:@MGL-SAMPLER-FUNCTION-SAMPLER MGL-PAX:SECTION)">4.1.1 Function Sampler</a></li>
</ul></li>
</ul></li>
<li><a href="#x-28MGL-RESAMPLE-3A-40MGL-RESAMPLE-20MGL-PAX-3ASECTION-29" title="(MGL-RESAMPLE:@MGL-RESAMPLE MGL-PAX:SECTION)">5 Resampling</a>

<ul>
<li><a href="#x-28MGL-RESAMPLE-3A-40MGL-RESAMPLE-PARTITIONS-20MGL-PAX-3ASECTION-29" title="(MGL-RESAMPLE:@MGL-RESAMPLE-PARTITIONS MGL-PAX:SECTION)">5.1 Partitions</a></li>
<li><a href="#x-28MGL-RESAMPLE-3A-40MGL-RESAMPLE-CROSS-VALIDATION-20MGL-PAX-3ASECTION-29" title="(MGL-RESAMPLE:@MGL-RESAMPLE-CROSS-VALIDATION MGL-PAX:SECTION)">5.2 Cross-validation</a></li>
<li><a href="#x-28MGL-RESAMPLE-3A-40MGL-RESAMPLE-BAGGING-20MGL-PAX-3ASECTION-29" title="(MGL-RESAMPLE:@MGL-RESAMPLE-BAGGING MGL-PAX:SECTION)">5.3 Bagging</a></li>
<li><a href="#x-28MGL-RESAMPLE-3A-40MGL-RESAMPLE-CV-BAGGING-20MGL-PAX-3ASECTION-29" title="(MGL-RESAMPLE:@MGL-RESAMPLE-CV-BAGGING MGL-PAX:SECTION)">5.4 CV Bagging</a></li>
<li><a href="#x-28MGL-RESAMPLE-3A-40MGL-RESAMPLE-MISC-20MGL-PAX-3ASECTION-29" title="(MGL-RESAMPLE:@MGL-RESAMPLE-MISC MGL-PAX:SECTION)">5.5 Miscellaneous Operations</a></li>
</ul></li>
<li><a href="#x-28MGL-OPT-3A-40MGL-OPT-20MGL-PAX-3ASECTION-29" title="(MGL-OPT:@MGL-OPT MGL-PAX:SECTION)">6 Gradient Based Optimization</a>

<ul>
<li><a href="#x-28MGL-OPT-3A-40MGL-OPT-EXTENSION-API-20MGL-PAX-3ASECTION-29" title="(MGL-OPT:@MGL-OPT-EXTENSION-API MGL-PAX:SECTION)">6.1 Extension API</a>

<ul>
<li><a href="#x-28MGL-OPT-3A-40MGL-OPT-OPTIMIZER-20MGL-PAX-3ASECTION-29" title="(MGL-OPT:@MGL-OPT-OPTIMIZER MGL-PAX:SECTION)">6.1.1 Implementing Optimizers</a></li>
<li><a href="#x-28MGL-OPT-3A-40MGL-OPT-GRADIENT-SOURCE-20MGL-PAX-3ASECTION-29" title="(MGL-OPT:@MGL-OPT-GRADIENT-SOURCE MGL-PAX:SECTION)">6.1.2 Implementing Gradient Sources</a></li>
<li><a href="#x-28MGL-OPT-3A-40MGL-OPT-GRADIENT-SINK-20MGL-PAX-3ASECTION-29" title="(MGL-OPT:@MGL-OPT-GRADIENT-SINK MGL-PAX:SECTION)">6.1.3 Implementing Gradient Sinks</a></li>
</ul></li>
<li><a href="#x-28MGL-OPT-3A-40MGL-OPT-ITERATIVE-OPTIMIZER-20MGL-PAX-3ASECTION-29" title="(MGL-OPT:@MGL-OPT-ITERATIVE-OPTIMIZER MGL-PAX:SECTION)">6.2 Iterative Optimizer</a></li>
<li><a href="#x-28MGL-GD-3A-40MGL-GD-20MGL-PAX-3ASECTION-29" title="(MGL-GD:@MGL-GD MGL-PAX:SECTION)">6.3 Gradient Descent</a>

<ul>
<li><a href="#x-28MGL-GD-3A-40MGL-GD-BATCH-GD-OPTIMIZER-20MGL-PAX-3ASECTION-29" title="(MGL-GD:@MGL-GD-BATCH-GD-OPTIMIZER MGL-PAX:SECTION)">6.3.1 Batch GD Optimizer</a></li>
<li><a href="#x-28MGL-GD-3A-40MGL-GD-SEGMENTED-GD-OPTIMIZER-20MGL-PAX-3ASECTION-29" title="(MGL-GD:@MGL-GD-SEGMENTED-GD-OPTIMIZER MGL-PAX:SECTION)">6.3.2 Segmented GD Optimizer</a></li>
<li><a href="#x-28MGL-GD-3A-40MGL-GD-PER-WEIGHT-OPTIMIZATION-20MGL-PAX-3ASECTION-29" title="(MGL-GD:@MGL-GD-PER-WEIGHT-OPTIMIZATION MGL-PAX:SECTION)">6.3.3 Per-weight Optimization</a></li>
</ul></li>
<li><a href="#x-28MGL-CG-3A-40MGL-CG-20MGL-PAX-3ASECTION-29" title="(MGL-CG:@MGL-CG MGL-PAX:SECTION)">6.4 Conjugate Gradient</a></li>
</ul></li>
<li><a href="#x-28MGL-DIFFUN-3A-40MGL-DIFFUN-20MGL-PAX-3ASECTION-29" title="(MGL-DIFFUN:@MGL-DIFFUN MGL-PAX:SECTION)">7 Differentiable Function</a></li>
<li><a href="#x-28MGL-3A-40MGL-BP-20MGL-PAX-3ASECTION-29" title="(MGL:@MGL-BP MGL-PAX:SECTION)">8 Backprogation Neural Networks</a></li>
<li><a href="#x-28MGL-3A-40MGL-BM-20MGL-PAX-3ASECTION-29" title="(MGL:@MGL-BM MGL-PAX:SECTION)">9 Boltzmann Machines</a></li>
<li><a href="#x-28MGL-3A-40MGL-GP-20MGL-PAX-3ASECTION-29" title="(MGL:@MGL-GP MGL-PAX:SECTION)">10 Gaussian Processes</a></li>
</ul>

<h6>[in package MGL]</h6>

<p><a name='x-28-22mgl-22-20ASDF-2FSYSTEM-3ASYSTEM-29'></a></p>

<h2><span class="navigation"> <a href="#x-28MGL-3A-40MGL-MANUAL-20MGL-PAX-3ASECTION-29" title="(MGL:@MGL-MANUAL MGL-PAX:SECTION)">&#8592;</a> <a href="#x-28MGL-3A-40MGL-MANUAL-20MGL-PAX-3ASECTION-29" title="(MGL:@MGL-MANUAL MGL-PAX:SECTION)">&#8593;</a> <a href="#x-28MGL-3A-40MGL-OVERVIEW-20MGL-PAX-3ASECTION-29" title="(MGL:@MGL-OVERVIEW MGL-PAX:SECTION)">&#8594;</a> <a href="#x-28-22mgl-22-20ASDF-2FSYSTEM-3ASYSTEM-29" title="(\&quot;mgl\&quot; ASDF/SYSTEM:SYSTEM)">&#8634;</a></span>1 mgl ASDF System Details</h2>

<ul>
<li>Version: 0.0.8</li>
<li>Description: MGL is a machine learning library for backpropagation
  neural networks, boltzmann machines, gaussian processes and more.</li>
<li>Licence: MIT, see COPYING.</li>
<li>Author: Gábor Melis</li>
<li>Mailto: <a href="mailto:mega@retes.hu" >mega@retes.hu</a></li>
<li>Homepage: <a href="http://quotenil.com" >http://quotenil.com</a></li>
</ul>

<p><a name='x-28MGL-3A-40MGL-OVERVIEW-20MGL-PAX-3ASECTION-29'></a></p>

<h2><span class="navigation"> <a href="#x-28-22mgl-22-20ASDF-2FSYSTEM-3ASYSTEM-29" title="(\&quot;mgl\&quot; ASDF/SYSTEM:SYSTEM)">&#8592;</a> <a href="#x-28MGL-3A-40MGL-MANUAL-20MGL-PAX-3ASECTION-29" title="(MGL:@MGL-MANUAL MGL-PAX:SECTION)">&#8593;</a> <a href="#x-28MGL-3A-40MGL-FEATURES-20MGL-PAX-3ASECTION-29" title="(MGL:@MGL-FEATURES MGL-PAX:SECTION)">&#8594;</a> <a href="#x-28MGL-3A-40MGL-OVERVIEW-20MGL-PAX-3ASECTION-29" title="(MGL:@MGL-OVERVIEW MGL-PAX:SECTION)">&#8634;</a></span>2 Overview</h2>

<p>MGL is a Common Lisp machine learning library by <a href="http://quotenil.com" >Gábor
Melis</a> with some parts originally contributed
by Ravenpack International. It implements:</p>

<ul>
<li><p>Backpropagation networks (BPN)</p>

<ul>
<li><p>Dropout</p></li>
<li><p>Rectified linear units</p></li>
<li><p>Maxout</p></li>
<li><p>Max-channel</p></li>
</ul></li>
<li><p>Boltzmann Machines</p></li>
<li><p>Restricted Boltzmann Machines (RBM)</p></li>
<li><p>Deep Belief Networks (DBN)</p></li>
<li><p>Semi Restricted Boltzmann Machines</p></li>
<li><p>Boltzmann Machines</p></li>
<li><p>Unrolling DBN to a BPN</p></li>
<li><p>Contrastive Divergence (CD) learning</p></li>
<li><p>Persistent Contrastive Divergence (PCD) learning</p></li>
<li><p>Gradient descent optimization</p></li>
<li><p>Nesterov momentum</p></li>
<li><p>Conjugate gradient optimization</p></li>
<li><p>Gaussian Processes</p></li>
<li><p>Optimizing Gaussian Processes as BPNs</p></li>
</ul>

<p><a name='x-28MGL-3A-40MGL-FEATURES-20MGL-PAX-3ASECTION-29'></a></p>

<h3><span class="navigation"> <a href="#x-28MGL-3A-40MGL-OVERVIEW-20MGL-PAX-3ASECTION-29" title="(MGL:@MGL-OVERVIEW MGL-PAX:SECTION)">&#8592;</a> <a href="#x-28MGL-3A-40MGL-OVERVIEW-20MGL-PAX-3ASECTION-29" title="(MGL:@MGL-OVERVIEW MGL-PAX:SECTION)">&#8593;</a> <a href="#x-28MGL-3A-40MGL-DEPENDENCIES-20MGL-PAX-3ASECTION-29" title="(MGL:@MGL-DEPENDENCIES MGL-PAX:SECTION)">&#8594;</a> <a href="#x-28MGL-3A-40MGL-FEATURES-20MGL-PAX-3ASECTION-29" title="(MGL:@MGL-FEATURES MGL-PAX:SECTION)">&#8634;</a></span>2.1 Features</h3>

<p>In general, the focus is on power and performance not on ease of use.
For example, it's possible to:</p>

<ul>
<li><p>control the order of presentation of training examples,</p></li>
<li><p>vary learning rate depending on time, state of the optimizer,</p></li>
<li><p>track all kinds of statistics during training,
etc.</p></li>
</ul>

<p>Perhaps one day there will be a cookie cutter interface with
restricted functionality if a reasonable compromise is found between
power and utility.</p>

<p><a name='x-28MGL-3A-40MGL-DEPENDENCIES-20MGL-PAX-3ASECTION-29'></a></p>

<h3><span class="navigation"> <a href="#x-28MGL-3A-40MGL-FEATURES-20MGL-PAX-3ASECTION-29" title="(MGL:@MGL-FEATURES MGL-PAX:SECTION)">&#8592;</a> <a href="#x-28MGL-3A-40MGL-OVERVIEW-20MGL-PAX-3ASECTION-29" title="(MGL:@MGL-OVERVIEW MGL-PAX:SECTION)">&#8593;</a> <a href="#x-28MGL-3A-40MGL-TESTS-20MGL-PAX-3ASECTION-29" title="(MGL:@MGL-TESTS MGL-PAX:SECTION)">&#8594;</a> <a href="#x-28MGL-3A-40MGL-DEPENDENCIES-20MGL-PAX-3ASECTION-29" title="(MGL:@MGL-DEPENDENCIES MGL-PAX:SECTION)">&#8634;</a></span>2.2 Dependencies</h3>

<p>MGL used to rely on <a href="https://github.com/tpapp/lla" >LLA</a> to
interface to BLAS and LAPACK. That's mostly history by now, but
configuration of foreign libraries is still done via <code>LLA</code>. See the
README in <code>LLA</code> on how to set things up. Note that these days OpenBLAS
is easier to set up and just as fast as ATLAS.</p>

<p><a href="https://github.com/takagi/cl-cuda" >CL-CUDA</a> is a dependency for
which the NVIDIA CUDA Toolkit needs to be installed, but MGL is
fully functional even if there is no cuda capable gpu installed. See
the <code>MGL-MAT:WITH-CUDA*</code> macro for how to use it.</p>

<p><a name='x-28MGL-3A-40MGL-TESTS-20MGL-PAX-3ASECTION-29'></a></p>

<h3><span class="navigation"> <a href="#x-28MGL-3A-40MGL-DEPENDENCIES-20MGL-PAX-3ASECTION-29" title="(MGL:@MGL-DEPENDENCIES MGL-PAX:SECTION)">&#8592;</a> <a href="#x-28MGL-3A-40MGL-OVERVIEW-20MGL-PAX-3ASECTION-29" title="(MGL:@MGL-OVERVIEW MGL-PAX:SECTION)">&#8593;</a> <a href="#x-28MGL-3A-40MGL-BUNDLED-SOFTWARE-20MGL-PAX-3ASECTION-29" title="(MGL:@MGL-BUNDLED-SOFTWARE MGL-PAX:SECTION)">&#8594;</a> <a href="#x-28MGL-3A-40MGL-TESTS-20MGL-PAX-3ASECTION-29" title="(MGL:@MGL-TESTS MGL-PAX:SECTION)">&#8634;</a></span>2.3 Tests</h3>

<p>Run the built in tests 
with:</p>

<pre><code>(ASDF:OOS 'ASDF:TEST-OP '#:MGL)
</code></pre>

<p>Note, that most of the tests are rather stochastic and can fail once
in a while.</p>

<p><a name='x-28MGL-3A-40MGL-BUNDLED-SOFTWARE-20MGL-PAX-3ASECTION-29'></a></p>

<h3><span class="navigation"> <a href="#x-28MGL-3A-40MGL-TESTS-20MGL-PAX-3ASECTION-29" title="(MGL:@MGL-TESTS MGL-PAX:SECTION)">&#8592;</a> <a href="#x-28MGL-3A-40MGL-OVERVIEW-20MGL-PAX-3ASECTION-29" title="(MGL:@MGL-OVERVIEW MGL-PAX:SECTION)">&#8593;</a> <a href="#x-28MGL-3A-40MGL-BASIC-CONCEPTS-20MGL-PAX-3ASECTION-29" title="(MGL:@MGL-BASIC-CONCEPTS MGL-PAX:SECTION)">&#8594;</a> <a href="#x-28MGL-3A-40MGL-BUNDLED-SOFTWARE-20MGL-PAX-3ASECTION-29" title="(MGL:@MGL-BUNDLED-SOFTWARE MGL-PAX:SECTION)">&#8634;</a></span>2.4 Bundled Software</h3>

<p>With <a href="https://github.com/melisgl/mgl-pax" >MGL-PAX</a> and
<a href="https://github.com/melisgl/mgl-mat" >MGL-MAT</a> libraries split off
there remains only a single library bundled with MGL which does
not depend on the rest of MGL:</p>

<ul>
<li><code>MGL-GNUPLOT</code>, a plotting library.</li>
</ul>

<p>There is also MGL-VISUALS which does depend on MGL.</p>

<p><a name='x-28MGL-3A-40MGL-BASIC-CONCEPTS-20MGL-PAX-3ASECTION-29'></a></p>

<h2><span class="navigation"> <a href="#x-28MGL-3A-40MGL-BUNDLED-SOFTWARE-20MGL-PAX-3ASECTION-29" title="(MGL:@MGL-BUNDLED-SOFTWARE MGL-PAX:SECTION)">&#8592;</a> <a href="#x-28MGL-3A-40MGL-MANUAL-20MGL-PAX-3ASECTION-29" title="(MGL:@MGL-MANUAL MGL-PAX:SECTION)">&#8593;</a> <a href="#x-28MGL-DATASET-3A-40MGL-DATASET-20MGL-PAX-3ASECTION-29" title="(MGL-DATASET:@MGL-DATASET MGL-PAX:SECTION)">&#8594;</a> <a href="#x-28MGL-3A-40MGL-BASIC-CONCEPTS-20MGL-PAX-3ASECTION-29" title="(MGL:@MGL-BASIC-CONCEPTS MGL-PAX:SECTION)">&#8634;</a></span>3 Basic Concepts</h2>

<p>MODEL, training set, test set, validation set,
sample/instance/example.</p>

<p><a name='x-28MGL-DATASET-3A-40MGL-DATASET-20MGL-PAX-3ASECTION-29'></a></p>

<h2><span class="navigation"> <a href="#x-28MGL-3A-40MGL-BASIC-CONCEPTS-20MGL-PAX-3ASECTION-29" title="(MGL:@MGL-BASIC-CONCEPTS MGL-PAX:SECTION)">&#8592;</a> <a href="#x-28MGL-3A-40MGL-MANUAL-20MGL-PAX-3ASECTION-29" title="(MGL:@MGL-MANUAL MGL-PAX:SECTION)">&#8593;</a> <a href="#x-28MGL-DATASET-3A-40MGL-SAMPLER-20MGL-PAX-3ASECTION-29" title="(MGL-DATASET:@MGL-SAMPLER MGL-PAX:SECTION)">&#8594;</a> <a href="#x-28MGL-DATASET-3A-40MGL-DATASET-20MGL-PAX-3ASECTION-29" title="(MGL-DATASET:@MGL-DATASET MGL-PAX:SECTION)">&#8634;</a></span>4 Dataset</h2>

<h6>[in package MGL-DATASET]</h6>

<p>Ultimately machine learning is about creating models of some
domain. The observations in the modelled domain are called
<em>instances</em>. Sets of instances are called <em>datasets</em>. Datasets are
used when fitting a model or when making predictions.</p>

<p>Implementationally speaking, an instance is typically represented by
a set of numbers which is called <em>feature vector</em>. A dataset is a
<code>SEQUENCE</code> of such instances or a <a href="#x-28MGL-DATASET-3A-40MGL-SAMPLER-20MGL-PAX-3ASECTION-29" title="(MGL-DATASET:@MGL-SAMPLER MGL-PAX:SECTION)">Sampler</a> object that produces
instances.</p>

<p><a name='x-28MGL-DATASET-3A-40MGL-SAMPLER-20MGL-PAX-3ASECTION-29'></a></p>

<h3><span class="navigation"> <a href="#x-28MGL-DATASET-3A-40MGL-DATASET-20MGL-PAX-3ASECTION-29" title="(MGL-DATASET:@MGL-DATASET MGL-PAX:SECTION)">&#8592;</a> <a href="#x-28MGL-DATASET-3A-40MGL-DATASET-20MGL-PAX-3ASECTION-29" title="(MGL-DATASET:@MGL-DATASET MGL-PAX:SECTION)">&#8593;</a> <a href="#x-28MGL-DATASET-3A-40MGL-SAMPLER-FUNCTION-SAMPLER-20MGL-PAX-3ASECTION-29" title="(MGL-DATASET:@MGL-SAMPLER-FUNCTION-SAMPLER MGL-PAX:SECTION)">&#8594;</a> <a href="#x-28MGL-DATASET-3A-40MGL-SAMPLER-20MGL-PAX-3ASECTION-29" title="(MGL-DATASET:@MGL-SAMPLER MGL-PAX:SECTION)">&#8634;</a></span>4.1 Sampler</h3>

<p>Some algorithms do not need random access to the entire dataset and
can work with a stream observations. Samplers are simple generators
providing two functions: <a href="#x-28MGL-DATASET-3ASAMPLE-20GENERIC-FUNCTION-29" title="(MGL-DATASET:SAMPLE GENERIC-FUNCTION)"><code>SAMPLE</code></a> and <a href="#x-28MGL-DATASET-3AFINISHEDP-20GENERIC-FUNCTION-29" title="(MGL-DATASET:FINISHEDP GENERIC-FUNCTION)"><code>FINISHEDP</code></a>.</p>

<p><a name='x-28MGL-DATASET-3ASAMPLE-20GENERIC-FUNCTION-29'></a></p>

<ul>
<li><p>[generic-function] <strong>SAMPLE</strong> <em>SAMPLER</em></p>

<p>If not <code>SAMPLER</code> has not run out of data (see
<a href="#x-28MGL-DATASET-3AFINISHEDP-20GENERIC-FUNCTION-29" title="(MGL-DATASET:FINISHEDP GENERIC-FUNCTION)"><code>FINISHEDP</code></a>) <a href="#x-28MGL-DATASET-3ASAMPLE-20GENERIC-FUNCTION-29" title="(MGL-DATASET:SAMPLE GENERIC-FUNCTION)"><code>SAMPLE</code></a> returns an object that represents a sample from
the world to be experienced or, in other words, simply something the
can be used as input for training or prediction.</p></li>
</ul>

<p><a name='x-28MGL-DATASET-3AFINISHEDP-20GENERIC-FUNCTION-29'></a></p>

<ul>
<li><p>[generic-function] <strong>FINISHEDP</strong> <em>SAMPLER</em></p>

<p>See if <code>SAMPLER</code> has run out of examples.</p></li>
</ul>

<p><a name='x-28MGL-DATASET-3ALIST-SAMPLES-20FUNCTION-29'></a></p>

<ul>
<li><p>[function] <strong>LIST-SAMPLES</strong> <em>SAMPLER MAX-SIZE</em></p>

<p>Return a list of samples of length at most <code>MAX-SIZE</code> or less if
<code>SAMPLER</code> runs out.</p></li>
</ul>

<p><a name='x-28MGL-DATASET-3AMAKE-SEQUENCE-SAMPLER-20FUNCTION-29'></a></p>

<ul>
<li><p>[function] <strong>MAKE-SEQUENCE-SAMPLER</strong> <em>SEQ</em></p>

<p>A simple sampler that returns elements of <code>SEQ</code> once, in order.</p></li>
</ul>

<p><a name='x-28MGL-DATASET-3A-2AINFINITELY-EMPTY-DATASET-2A-20VARIABLE-29'></a></p>

<ul>
<li><p>[variable] <strong>*INFINITELY-EMPTY-DATASET*</strong> <em>#&lt;FUNCTION-SAMPLER &quot;infinitely empty&quot; &gt;</em></p>

<p>This is the default dataset for <a href="#x-28MGL-OPT-3AMINIMIZE-20FUNCTION-29" title="(MGL-OPT:MINIMIZE FUNCTION)"><code>MGL-OPT:MINIMIZE</code></a>. It's an infinite
stream of NILs.</p></li>
</ul>

<p><a name='x-28MGL-DATASET-3A-40MGL-SAMPLER-FUNCTION-SAMPLER-20MGL-PAX-3ASECTION-29'></a></p>

<h4><span class="navigation"> <a href="#x-28MGL-DATASET-3A-40MGL-SAMPLER-20MGL-PAX-3ASECTION-29" title="(MGL-DATASET:@MGL-SAMPLER MGL-PAX:SECTION)">&#8592;</a> <a href="#x-28MGL-DATASET-3A-40MGL-SAMPLER-20MGL-PAX-3ASECTION-29" title="(MGL-DATASET:@MGL-SAMPLER MGL-PAX:SECTION)">&#8593;</a> <a href="#x-28MGL-RESAMPLE-3A-40MGL-RESAMPLE-20MGL-PAX-3ASECTION-29" title="(MGL-RESAMPLE:@MGL-RESAMPLE MGL-PAX:SECTION)">&#8594;</a> <a href="#x-28MGL-DATASET-3A-40MGL-SAMPLER-FUNCTION-SAMPLER-20MGL-PAX-3ASECTION-29" title="(MGL-DATASET:@MGL-SAMPLER-FUNCTION-SAMPLER MGL-PAX:SECTION)">&#8634;</a></span>4.1.1 Function Sampler</h4>

<p><a name='x-28MGL-DATASET-3AFUNCTION-SAMPLER-20CLASS-29'></a></p>

<ul>
<li><p>[class] <strong>FUNCTION-SAMPLER</strong></p>

<p>A sampler with a function in its <a href="#x-28MGL-DATASET-3ASAMPLER-20-28MGL-PAX-3AREADER-20MGL-DATASET-3AFUNCTION-SAMPLER-29-29" title="(MGL-DATASET:SAMPLER (MGL-PAX:READER MGL-DATASET:FUNCTION-SAMPLER))"><code>SAMPLER</code></a> that
produces a stream of samples which may or may not be finite
depending on <a href="#x-28MGL-DATASET-3AMAX-N-SAMPLES-20-28MGL-PAX-3AACCESSOR-20MGL-DATASET-3AFUNCTION-SAMPLER-29-29" title="(MGL-DATASET:MAX-N-SAMPLES (MGL-PAX:ACCESSOR MGL-DATASET:FUNCTION-SAMPLER))"><code>MAX-N-SAMPLES</code></a>. <a href="#x-28MGL-DATASET-3AFINISHEDP-20GENERIC-FUNCTION-29" title="(MGL-DATASET:FINISHEDP GENERIC-FUNCTION)"><code>FINISHEDP</code></a> returns <code>T</code> iff <a href="#x-28MGL-DATASET-3AMAX-N-SAMPLES-20-28MGL-PAX-3AACCESSOR-20MGL-DATASET-3AFUNCTION-SAMPLER-29-29" title="(MGL-DATASET:MAX-N-SAMPLES (MGL-PAX:ACCESSOR MGL-DATASET:FUNCTION-SAMPLER))"><code>MAX-N-SAMPLES</code></a> is
non-nil, and it's not greater than the number of samples
generated (<a href="#x-28MGL-DATASET-3AN-SAMPLES-20-28MGL-PAX-3AREADER-20MGL-DATASET-3AFUNCTION-SAMPLER-29-29" title="(MGL-DATASET:N-SAMPLES (MGL-PAX:READER MGL-DATASET:FUNCTION-SAMPLER))"><code>N-SAMPLES</code></a>).</p>

<pre><code>(list-samples (make-instance 'function-sampler
                             :sampler (lambda ()
                                        (random 10))
                             :max-n-samples 5)
              10)
=&gt; (3 5 2 3 3)
</code></pre></li>
</ul>

<p><a name='x-28MGL-DATASET-3ASAMPLER-20-28MGL-PAX-3AREADER-20MGL-DATASET-3AFUNCTION-SAMPLER-29-29'></a></p>

<ul>
<li><p>[reader] <strong>SAMPLER</strong> <em>FUNCTION-SAMPLER</em> <em>(:SAMPLER)</em></p>

<p>A generator function of no arguments that returns
the next sample.</p></li>
</ul>

<p><a name='x-28MGL-DATASET-3AMAX-N-SAMPLES-20-28MGL-PAX-3AACCESSOR-20MGL-DATASET-3AFUNCTION-SAMPLER-29-29'></a></p>

<ul>
<li>[accessor] <strong>MAX-N-SAMPLES</strong> <em>FUNCTION-SAMPLER</em> <em>(:MAX-N-SAMPLES = NIL)</em></li>
</ul>

<p><a name='x-28MGL-COMMON-3ANAME-20-28MGL-PAX-3AREADER-20MGL-DATASET-3AFUNCTION-SAMPLER-29-29'></a></p>

<ul>
<li><p>[reader] <strong>NAME</strong> <em>FUNCTION-SAMPLER</em> <em>(:NAME = NIL)</em></p>

<p>An arbitrary object naming the sampler. Only used
for printing the sampler object.</p></li>
</ul>

<p><a name='x-28MGL-DATASET-3AN-SAMPLES-20-28MGL-PAX-3AREADER-20MGL-DATASET-3AFUNCTION-SAMPLER-29-29'></a></p>

<ul>
<li>[reader] <strong>N-SAMPLES</strong> <em>FUNCTION-SAMPLER</em> <em>(:N-SAMPLES = 0)</em></li>
</ul>

<p><a name='x-28MGL-RESAMPLE-3A-40MGL-RESAMPLE-20MGL-PAX-3ASECTION-29'></a></p>

<h2><span class="navigation"> <a href="#x-28MGL-DATASET-3A-40MGL-SAMPLER-FUNCTION-SAMPLER-20MGL-PAX-3ASECTION-29" title="(MGL-DATASET:@MGL-SAMPLER-FUNCTION-SAMPLER MGL-PAX:SECTION)">&#8592;</a> <a href="#x-28MGL-3A-40MGL-MANUAL-20MGL-PAX-3ASECTION-29" title="(MGL:@MGL-MANUAL MGL-PAX:SECTION)">&#8593;</a> <a href="#x-28MGL-RESAMPLE-3A-40MGL-RESAMPLE-PARTITIONS-20MGL-PAX-3ASECTION-29" title="(MGL-RESAMPLE:@MGL-RESAMPLE-PARTITIONS MGL-PAX:SECTION)">&#8594;</a> <a href="#x-28MGL-RESAMPLE-3A-40MGL-RESAMPLE-20MGL-PAX-3ASECTION-29" title="(MGL-RESAMPLE:@MGL-RESAMPLE MGL-PAX:SECTION)">&#8634;</a></span>5 Resampling</h2>

<h6>[in package MGL-RESAMPLE]</h6>

<p>The focus of this package is on resampling methods such as
cross-validation and bagging which can be used for model evaluation,
model selection, and also as a simple form of ensembling. Data
partitioning and sampling functions are also provided because they
tend to be used together with resampling.</p>

<p><a name='x-28MGL-RESAMPLE-3A-40MGL-RESAMPLE-PARTITIONS-20MGL-PAX-3ASECTION-29'></a></p>

<h3><span class="navigation"> <a href="#x-28MGL-RESAMPLE-3A-40MGL-RESAMPLE-20MGL-PAX-3ASECTION-29" title="(MGL-RESAMPLE:@MGL-RESAMPLE MGL-PAX:SECTION)">&#8592;</a> <a href="#x-28MGL-RESAMPLE-3A-40MGL-RESAMPLE-20MGL-PAX-3ASECTION-29" title="(MGL-RESAMPLE:@MGL-RESAMPLE MGL-PAX:SECTION)">&#8593;</a> <a href="#x-28MGL-RESAMPLE-3A-40MGL-RESAMPLE-CROSS-VALIDATION-20MGL-PAX-3ASECTION-29" title="(MGL-RESAMPLE:@MGL-RESAMPLE-CROSS-VALIDATION MGL-PAX:SECTION)">&#8594;</a> <a href="#x-28MGL-RESAMPLE-3A-40MGL-RESAMPLE-PARTITIONS-20MGL-PAX-3ASECTION-29" title="(MGL-RESAMPLE:@MGL-RESAMPLE-PARTITIONS MGL-PAX:SECTION)">&#8634;</a></span>5.1 Partitions</h3>

<p>The following functions partition a dataset (currently only
SEQUENCEs are supported) into a number of partitions. For each
element in the original dataset there is exactly one partition that
contains it.</p>

<p><a name='x-28MGL-RESAMPLE-3AFRACTURE-20FUNCTION-29'></a></p>

<ul>
<li><p>[function] <strong>FRACTURE</strong> <em>FRACTIONS SEQ &amp;KEY WEIGHT</em></p>

<p>Partition <code>SEQ</code> into a number of subsequences. <code>FRACTIONS</code> is either a
positive integer or a list of non-negative real numbers. <code>WEIGHT</code> is
<code>NIL</code> or a function that returns a non-negative real number when
called with an element from <code>SEQ</code>. If <code>FRACTIONS</code> is a positive integer
then return a list of that many subsequences with equal sum of
weights bar rounding errors, else partition <code>SEQ</code> into subsequences,
where the sum of weights of subsequence I is proportional to element
I of <code>FRACTIONS</code>. If <code>WEIGHT</code> is <code>NIL</code>, then it's element is assumed to
have the same weight.</p>

<p>To split into 5 sequences:</p>

<pre><code>(fracture 5 '(0 1 2 3 4 5 6 7 8 9))
=&gt; ((0 1) (2 3) (4 5) (6 7) (8 9))
</code></pre>

<p>To split into two sequences whose lengths are proportional to 2 and
3:</p>

<pre><code>(fracture '(2 3) '(0 1 2 3 4 5 6 7 8 9))
=&gt; ((0 1 2 3) (4 5 6 7 8 9))
</code></pre></li>
</ul>

<p><a name='x-28MGL-RESAMPLE-3ASTRATIFY-20FUNCTION-29'></a></p>

<ul>
<li><p>[function] <strong>STRATIFY</strong> <em>SEQ &amp;KEY (KEY #'IDENTITY) (TEST #'EQL)</em></p>

<p>Return the list of strata of <code>SEQ</code>. <code>SEQ</code> is sequence of elements for
which the function <code>KEY</code> returns the class they belong to. Such
classes are opaque objects compared for equality with <code>TEST</code>. A
stratum is a sequence of elements with the same (under <code>TEST</code>) <code>KEY</code>.</p>

<pre><code>(stratify '(0 1 2 3 4 5 6 7 8 9) :key #'evenp)
=&gt; ((0 2 4 6 8) (1 3 5 7 9))
</code></pre></li>
</ul>

<p><a name='x-28MGL-RESAMPLE-3AFRACTURE-STRATIFIED-20FUNCTION-29'></a></p>

<ul>
<li><p>[function] <strong>FRACTURE-STRATIFIED</strong> <em>FRACTIONS SEQ &amp;KEY (KEY #'IDENTITY) (TEST #'EQL) WEIGHT</em></p>

<p>Similar to <a href="#x-28MGL-RESAMPLE-3AFRACTURE-20FUNCTION-29" title="(MGL-RESAMPLE:FRACTURE FUNCTION)"><code>FRACTURE</code></a>, but also makes sure that keys are evenly
distributed among the partitions (see <a href="#x-28MGL-RESAMPLE-3ASTRATIFY-20FUNCTION-29" title="(MGL-RESAMPLE:STRATIFY FUNCTION)"><code>STRATIFY</code></a>). It can be useful
for classification tasks to partition the data set while keeping the
distribution of classes the same.</p>

<p>Note that the sets returned are not in random order. In fact, they
are sorted internally by <code>KEY</code>.</p>

<p>For example, to make two splits with approximately the same number
of even and odd numbers:</p>

<pre><code>(fracture-stratified 2 '(0 1 2 3 4 5 6 7 8 9) :key #'evenp)
=&gt; ((0 2 1 3) (4 6 8 5 7 9))
</code></pre></li>
</ul>

<p><a name='x-28MGL-RESAMPLE-3A-40MGL-RESAMPLE-CROSS-VALIDATION-20MGL-PAX-3ASECTION-29'></a></p>

<h3><span class="navigation"> <a href="#x-28MGL-RESAMPLE-3A-40MGL-RESAMPLE-PARTITIONS-20MGL-PAX-3ASECTION-29" title="(MGL-RESAMPLE:@MGL-RESAMPLE-PARTITIONS MGL-PAX:SECTION)">&#8592;</a> <a href="#x-28MGL-RESAMPLE-3A-40MGL-RESAMPLE-20MGL-PAX-3ASECTION-29" title="(MGL-RESAMPLE:@MGL-RESAMPLE MGL-PAX:SECTION)">&#8593;</a> <a href="#x-28MGL-RESAMPLE-3A-40MGL-RESAMPLE-BAGGING-20MGL-PAX-3ASECTION-29" title="(MGL-RESAMPLE:@MGL-RESAMPLE-BAGGING MGL-PAX:SECTION)">&#8594;</a> <a href="#x-28MGL-RESAMPLE-3A-40MGL-RESAMPLE-CROSS-VALIDATION-20MGL-PAX-3ASECTION-29" title="(MGL-RESAMPLE:@MGL-RESAMPLE-CROSS-VALIDATION MGL-PAX:SECTION)">&#8634;</a></span>5.2 Cross-validation</h3>

<p><a name='x-28MGL-RESAMPLE-3ACROSS-VALIDATE-20FUNCTION-29'></a></p>

<ul>
<li><p>[function] <strong>CROSS-VALIDATE</strong> <em>DATA FN &amp;KEY (N-FOLDS 5) (FOLDS (ALEXANDRIA.0.DEV:IOTA N-FOLDS)) (SPLIT-FN #'SPLIT-FOLD/MOD) PASS-FOLD</em></p>

<p>Map <code>FN</code> over the <code>FOLDS</code> of <code>DATA</code> split with <code>SPLIT-FN</code> and collect the
results in a list. The simplest demonstration is:</p>

<pre><code>(cross-validate '(0 1 2 3 4)
                (lambda (test training)
                 (list test training))
                :n-folds 5)
=&gt; (((0) (1 2 3 4))
    ((1) (0 2 3 4))
    ((2) (0 1 3 4))
    ((3) (0 1 2 4))
    ((4) (0 1 2 3)))
</code></pre>

<p>Of course, in practice one would typically train a model and return
the trained model and/or its score on <code>TEST</code>. Also, sometimes one may
want to do only some of the folds and remember which ones they were:</p>

<pre><code>(cross-validate '(0 1 2 3 4)
                (lambda (fold test training)
                 (list :fold fold test training))
                :folds '(2 3)
                :pass-fold t)
=&gt; ((:fold 2 (2) (0 1 3 4))
    (:fold 3 (3) (0 1 2 4)))
</code></pre>

<p>Finally, the way the data is split can be customized. By default
<a href="#x-28MGL-RESAMPLE-3ASPLIT-FOLD-2FMOD-20FUNCTION-29" title="(MGL-RESAMPLE:SPLIT-FOLD/MOD FUNCTION)"><code>SPLIT-FOLD/MOD</code></a> is called with the arguments <code>DATA</code>, the fold (from
among <code>FOLDS</code>) and <code>N-FOLDS</code>. <a href="#x-28MGL-RESAMPLE-3ASPLIT-FOLD-2FMOD-20FUNCTION-29" title="(MGL-RESAMPLE:SPLIT-FOLD/MOD FUNCTION)"><code>SPLIT-FOLD/MOD</code></a> returns two values which
are then passed on to <code>FN</code>. One can use <a href="#x-28MGL-RESAMPLE-3ASPLIT-FOLD-2FCONT-20FUNCTION-29" title="(MGL-RESAMPLE:SPLIT-FOLD/CONT FUNCTION)"><code>SPLIT-FOLD/CONT</code></a> or
<a href="#x-28MGL-RESAMPLE-3ASPLIT-STRATIFIED-20FUNCTION-29" title="(MGL-RESAMPLE:SPLIT-STRATIFIED FUNCTION)"><code>SPLIT-STRATIFIED</code></a> or any other function that works with these
arguments. The only real constraint is that <code>FN</code> has to take as many
arguments (plus the fold argument if <code>PASS-FOLD</code>) as <code>SPLIT-FN</code>
returns.</p></li>
</ul>

<p><a name='x-28MGL-RESAMPLE-3ASPLIT-FOLD-2FMOD-20FUNCTION-29'></a></p>

<ul>
<li><p>[function] <strong>SPLIT-FOLD/MOD</strong> <em>SEQ FOLD N-FOLDS</em></p>

<p>Partition <code>SEQ</code> into two sequences: one with elements of <code>SEQ</code> with
indices whose remainder is <code>FOLD</code> when divided with <code>N-FOLDS</code>, and a
second one with the rest. The second one is the larger set. The
order of elements remains stable. This function is suitable as the
<code>SPLIT-FN</code> argument of <a href="#x-28MGL-RESAMPLE-3ACROSS-VALIDATE-20FUNCTION-29" title="(MGL-RESAMPLE:CROSS-VALIDATE FUNCTION)"><code>CROSS-VALIDATE</code></a>.</p></li>
</ul>

<p><a name='x-28MGL-RESAMPLE-3ASPLIT-FOLD-2FCONT-20FUNCTION-29'></a></p>

<ul>
<li><p>[function] <strong>SPLIT-FOLD/CONT</strong> <em>SEQ FOLD N-FOLDS</em></p>

<p>Imagine dividing <code>SEQ</code> into <code>N-FOLDS</code> subsequences of the same
size (bar rounding). Return the subsequence of index <code>FOLD</code> as the
first value and the all the other subsequences concatenated into one
as the second value. The order of elements remains stable. This
function is suitable as the <code>SPLIT-FN</code> argument of <a href="#x-28MGL-RESAMPLE-3ACROSS-VALIDATE-20FUNCTION-29" title="(MGL-RESAMPLE:CROSS-VALIDATE FUNCTION)"><code>CROSS-VALIDATE</code></a>.</p></li>
</ul>

<p><a name='x-28MGL-RESAMPLE-3ASPLIT-STRATIFIED-20FUNCTION-29'></a></p>

<ul>
<li><p>[function] <strong>SPLIT-STRATIFIED</strong> <em>SEQ FOLD N-FOLDS &amp;KEY (KEY #'IDENTITY) (TEST #'EQL) WEIGHT</em></p>

<p>Split <code>SEQ</code> into <code>N-FOLDS</code> partitions (as in <a href="#x-28MGL-RESAMPLE-3AFRACTURE-STRATIFIED-20FUNCTION-29" title="(MGL-RESAMPLE:FRACTURE-STRATIFIED FUNCTION)"><code>FRACTURE-STRATIFIED</code></a>).
Return the partition of index <code>FOLD</code> as the first value, and the
concatenation of the rest as the second value. This function is
suitable as the <code>SPLIT-FN</code> argument of <a href="#x-28MGL-RESAMPLE-3ACROSS-VALIDATE-20FUNCTION-29" title="(MGL-RESAMPLE:CROSS-VALIDATE FUNCTION)"><code>CROSS-VALIDATE</code></a> (mostly likely
as a closure with <code>KEY</code>, <code>TEST</code>, <code>WEIGHT</code> bound).</p></li>
</ul>

<p><a name='x-28MGL-RESAMPLE-3A-40MGL-RESAMPLE-BAGGING-20MGL-PAX-3ASECTION-29'></a></p>

<h3><span class="navigation"> <a href="#x-28MGL-RESAMPLE-3A-40MGL-RESAMPLE-CROSS-VALIDATION-20MGL-PAX-3ASECTION-29" title="(MGL-RESAMPLE:@MGL-RESAMPLE-CROSS-VALIDATION MGL-PAX:SECTION)">&#8592;</a> <a href="#x-28MGL-RESAMPLE-3A-40MGL-RESAMPLE-20MGL-PAX-3ASECTION-29" title="(MGL-RESAMPLE:@MGL-RESAMPLE MGL-PAX:SECTION)">&#8593;</a> <a href="#x-28MGL-RESAMPLE-3A-40MGL-RESAMPLE-CV-BAGGING-20MGL-PAX-3ASECTION-29" title="(MGL-RESAMPLE:@MGL-RESAMPLE-CV-BAGGING MGL-PAX:SECTION)">&#8594;</a> <a href="#x-28MGL-RESAMPLE-3A-40MGL-RESAMPLE-BAGGING-20MGL-PAX-3ASECTION-29" title="(MGL-RESAMPLE:@MGL-RESAMPLE-BAGGING MGL-PAX:SECTION)">&#8634;</a></span>5.3 Bagging</h3>

<p><a name='x-28MGL-RESAMPLE-3ABAG-20FUNCTION-29'></a></p>

<ul>
<li><p>[function] <strong>BAG</strong> <em>SEQ FN &amp;KEY (RATIO 1) N WEIGHT (REPLACEMENT T) KEY (TEST #'EQL) (RANDOM-STATE *RANDOM-STATE*)</em></p>

<p>Sample from <code>SEQ</code> with <a href="#x-28MGL-RESAMPLE-3ASAMPLE-FROM-20FUNCTION-29" title="(MGL-RESAMPLE:SAMPLE-FROM FUNCTION)"><code>SAMPLE-FROM</code></a> (passing <code>RATIO</code>, <code>WEIGHT</code>,
<code>REPLACEMENT</code>), or <a href="#x-28MGL-RESAMPLE-3ASAMPLE-STRATIFIED-20FUNCTION-29" title="(MGL-RESAMPLE:SAMPLE-STRATIFIED FUNCTION)"><code>SAMPLE-STRATIFIED</code></a> if <code>KEY</code> is not <code>NIL</code>. Call <code>FN</code> with
the sample. If <code>N</code> is <code>NIL</code> then keep repeating this until <code>FN</code> performs a
non-local exit. Else <code>N</code> must be a non-negative integer, <code>N</code> iterations
will be performed, the primary values returned by <code>FN</code> collected into
a list and returned. See <a href="#x-28MGL-RESAMPLE-3ASAMPLE-FROM-20FUNCTION-29" title="(MGL-RESAMPLE:SAMPLE-FROM FUNCTION)"><code>SAMPLE-FROM</code></a> and <a href="#x-28MGL-RESAMPLE-3ASAMPLE-STRATIFIED-20FUNCTION-29" title="(MGL-RESAMPLE:SAMPLE-STRATIFIED FUNCTION)"><code>SAMPLE-STRATIFIED</code></a> for
examples.</p></li>
</ul>

<p><a name='x-28MGL-RESAMPLE-3ASAMPLE-FROM-20FUNCTION-29'></a></p>

<ul>
<li><p>[function] <strong>SAMPLE-FROM</strong> <em>RATIO SEQ &amp;KEY WEIGHT REPLACEMENT (RANDOM-STATE *RANDOM-STATE*)</em></p>

<p>Return a sequence constructed by sampling with or without
<code>REPLACEMENT</code> from <code>SEQ</code>. The sum of weights in the result sequence will
approximately be the sum of weights of <code>SEQ</code> times <code>RATIO</code>. If <code>WEIGHT</code> is
<code>NIL</code> then elements are assumed to have equal weights, else <code>WEIGHT</code>
should return a non-negative real number when called with an element
of <code>SEQ</code>.</p>

<p>To randomly select half of the elements:</p>

<pre><code>(sample-from 1/2 '(0 1 2 3 4 5))
=&gt; (5 3 2)
</code></pre>

<p>To randomly select some elements such that the sum of their weights
constitute about half of the sum of weights across the whole
sequence:</p>

<pre><code>(sample-from 1/2 '(0 1 2 3 4 5 6 7 8 9) :weight #'identity)
=&gt; (9 4 1 6 8) ; sums to 28 that's near 45/2
</code></pre>

<p>To sample with replacement (that is, allowing the element to be
sampled multiple times):</p>

<pre><code>(sample-from 1 '(0 1 2 3 4 5) :replacement t)
=&gt; (1 1 5 1 4 4)
</code></pre></li>
</ul>

<p><a name='x-28MGL-RESAMPLE-3ASAMPLE-STRATIFIED-20FUNCTION-29'></a></p>

<ul>
<li><p>[function] <strong>SAMPLE-STRATIFIED</strong> <em>RATIO SEQ &amp;KEY WEIGHT REPLACEMENT (KEY #'IDENTITY) (TEST #'EQL) (RANDOM-STATE *RANDOM-STATE*)</em></p>

<p>Like <a href="#x-28MGL-RESAMPLE-3ASAMPLE-FROM-20FUNCTION-29" title="(MGL-RESAMPLE:SAMPLE-FROM FUNCTION)"><code>SAMPLE-FROM</code></a> but makes sure that the weighted proportion of
classes in the result is approximately the same as the proportion in
<code>SEQ</code>. See <a href="#x-28MGL-RESAMPLE-3ASTRATIFY-20FUNCTION-29" title="(MGL-RESAMPLE:STRATIFY FUNCTION)"><code>STRATIFY</code></a> for the description of <code>KEY</code> and <code>TEST</code>.</p></li>
</ul>

<p><a name='x-28MGL-RESAMPLE-3A-40MGL-RESAMPLE-CV-BAGGING-20MGL-PAX-3ASECTION-29'></a></p>

<h3><span class="navigation"> <a href="#x-28MGL-RESAMPLE-3A-40MGL-RESAMPLE-BAGGING-20MGL-PAX-3ASECTION-29" title="(MGL-RESAMPLE:@MGL-RESAMPLE-BAGGING MGL-PAX:SECTION)">&#8592;</a> <a href="#x-28MGL-RESAMPLE-3A-40MGL-RESAMPLE-20MGL-PAX-3ASECTION-29" title="(MGL-RESAMPLE:@MGL-RESAMPLE MGL-PAX:SECTION)">&#8593;</a> <a href="#x-28MGL-RESAMPLE-3A-40MGL-RESAMPLE-MISC-20MGL-PAX-3ASECTION-29" title="(MGL-RESAMPLE:@MGL-RESAMPLE-MISC MGL-PAX:SECTION)">&#8594;</a> <a href="#x-28MGL-RESAMPLE-3A-40MGL-RESAMPLE-CV-BAGGING-20MGL-PAX-3ASECTION-29" title="(MGL-RESAMPLE:@MGL-RESAMPLE-CV-BAGGING MGL-PAX:SECTION)">&#8634;</a></span>5.4 CV Bagging</h3>

<p><a name='x-28MGL-RESAMPLE-3ABAG-CV-20FUNCTION-29'></a></p>

<ul>
<li><p>[function] <strong>BAG-CV</strong> <em>DATA FN &amp;KEY N (N-FOLDS 5) (FOLDS (ALEXANDRIA.0.DEV:IOTA N-FOLDS)) (SPLIT-FN #'SPLIT-FOLD/MOD) PASS-FOLD (RANDOM-STATE *RANDOM-STATE*)</em></p>

<p>Perform cross-validation on different shuffles of <code>DATA</code> <code>N</code> times and
collect the results. Since <a href="#x-28MGL-RESAMPLE-3ACROSS-VALIDATE-20FUNCTION-29" title="(MGL-RESAMPLE:CROSS-VALIDATE FUNCTION)"><code>CROSS-VALIDATE</code></a> collects the return values
of <code>FN</code>, the return value of this function is a list of lists of <code>FN</code>
results. If <code>N</code> is <code>NIL</code>, don't collect anything just keep doing
repeated CVs until <code>FN</code> performs an non-local exit.</p>

<p>The following example simply collects the test and training sets for
2-fold CV repeated 3 times with shuffled data:</p>

<pre><code> (bag-cv '(0 1 2 3 4) #'list :n 3 :n-folds 2)
 =&gt; ((((2 3 4) (1 0))
      ((1 0) (2 3 4)))
     (((2 1 0) (4 3))
      ((4 3) (2 1 0)))
     (((1 0 3) (2 4))
      ((2 4) (1 0 3))))
</code></pre>

<p>CV bagging is useful when a single CV is not producing stable
results. As an ensemble method, CV bagging has the advantage over
bagging that each example will occur the same number of times and
after the first CV is complete there is a complete but less reliable
estimate for each example which gets refined by further CVs.</p></li>
</ul>

<p><a name='x-28MGL-RESAMPLE-3A-40MGL-RESAMPLE-MISC-20MGL-PAX-3ASECTION-29'></a></p>

<h3><span class="navigation"> <a href="#x-28MGL-RESAMPLE-3A-40MGL-RESAMPLE-CV-BAGGING-20MGL-PAX-3ASECTION-29" title="(MGL-RESAMPLE:@MGL-RESAMPLE-CV-BAGGING MGL-PAX:SECTION)">&#8592;</a> <a href="#x-28MGL-RESAMPLE-3A-40MGL-RESAMPLE-20MGL-PAX-3ASECTION-29" title="(MGL-RESAMPLE:@MGL-RESAMPLE MGL-PAX:SECTION)">&#8593;</a> <a href="#x-28MGL-OPT-3A-40MGL-OPT-20MGL-PAX-3ASECTION-29" title="(MGL-OPT:@MGL-OPT MGL-PAX:SECTION)">&#8594;</a> <a href="#x-28MGL-RESAMPLE-3A-40MGL-RESAMPLE-MISC-20MGL-PAX-3ASECTION-29" title="(MGL-RESAMPLE:@MGL-RESAMPLE-MISC MGL-PAX:SECTION)">&#8634;</a></span>5.5 Miscellaneous Operations</h3>

<p><a name='x-28MGL-RESAMPLE-3ASPREAD-STRATA-20FUNCTION-29'></a></p>

<ul>
<li><p>[function] <strong>SPREAD-STRATA</strong> <em>SEQ &amp;KEY (KEY #'IDENTITY) (TEST #'EQL)</em></p>

<p>Return a sequence that's a reordering of <code>SEQ</code> such that elements
belonging to different strata (under <code>KEY</code> and <code>TEST</code>, see <a href="#x-28MGL-RESAMPLE-3ASTRATIFY-20FUNCTION-29" title="(MGL-RESAMPLE:STRATIFY FUNCTION)"><code>STRATIFY</code></a>) are
distributed evenly. The order of elements belonging to the same
stratum is unchanged.</p>

<p>For example, to make sure that even and odd numbers are distributed
evenly:</p>

<pre><code>(spread-strata '(0 2 4 6 8 1 3 5 7 9) :key #'evenp)
=&gt; (0 1 2 3 4 5 6 7 8 9)
</code></pre>

<p>Same thing with unbalanced classes:</p>

<pre><code>(spread-strata (vector 0 2 3 5 6 1 4)
               :key (lambda (x)
                      (if (member x '(1 4))
                          t
                          nil)))
=&gt; #(0 1 2 3 4 5 6)
</code></pre></li>
</ul>

<p><a name='x-28MGL-RESAMPLE-3AZIP-EVENLY-20FUNCTION-29'></a></p>

<ul>
<li><p>[function] <strong>ZIP-EVENLY</strong> <em>SEQS &amp;KEY RESULT-TYPE</em></p>

<p>Make a single sequence out of the sequences in <code>SEQS</code> so that in the
returned sequence indices of elements belonging to the same source
sequence are spread evenly across the whole range. The result is a
list is <code>RESULT-TYPE</code> is <code>LIST</code>, it's a vector if <code>RESULT-TYPE</code> is <code>VECTOR</code>.
If <code>RESULT-TYPE</code> is <code>NIL</code>, then it's determined by the type of the first
sequence in <code>SEQS</code>.</p>

<pre><code>(zip-evenly '((0 2 4) (1 3)))
=&gt; (0 1 2 3 4)
</code></pre></li>
</ul>

<p><a name='x-28MGL-OPT-3A-40MGL-OPT-20MGL-PAX-3ASECTION-29'></a></p>

<h2><span class="navigation"> <a href="#x-28MGL-RESAMPLE-3A-40MGL-RESAMPLE-MISC-20MGL-PAX-3ASECTION-29" title="(MGL-RESAMPLE:@MGL-RESAMPLE-MISC MGL-PAX:SECTION)">&#8592;</a> <a href="#x-28MGL-3A-40MGL-MANUAL-20MGL-PAX-3ASECTION-29" title="(MGL:@MGL-MANUAL MGL-PAX:SECTION)">&#8593;</a> <a href="#x-28MGL-OPT-3A-40MGL-OPT-EXTENSION-API-20MGL-PAX-3ASECTION-29" title="(MGL-OPT:@MGL-OPT-EXTENSION-API MGL-PAX:SECTION)">&#8594;</a> <a href="#x-28MGL-OPT-3A-40MGL-OPT-20MGL-PAX-3ASECTION-29" title="(MGL-OPT:@MGL-OPT MGL-PAX:SECTION)">&#8634;</a></span>6 Gradient Based Optimization</h2>

<h6>[in package MGL-OPT]</h6>

<p>We have a real valued, differentiable function F and the task is to
find the parameters that minimize its value. Optimization starts
from a single point in the parameter space of F, and this single
point is updated iteratively based on the gradient and value of F at
or around the current point.</p>

<p>Note that while the stated problem is that of global optimization,
for non-convex functions, most algorithms will tend to converge to a
local optimum.</p>

<p>Currently, there are two optimization algorithms:
<a href="#x-28MGL-GD-3A-40MGL-GD-20MGL-PAX-3ASECTION-29" title="(MGL-GD:@MGL-GD MGL-PAX:SECTION)">Gradient Descent</a> (with several variants) and <a href="#x-28MGL-CG-3A-40MGL-CG-20MGL-PAX-3ASECTION-29" title="(MGL-CG:@MGL-CG MGL-PAX:SECTION)">Conjugate Gradient</a> both of
which are first order methods (they do not need second order
gradients) but more can be added with the <a href="#x-28MGL-OPT-3A-40MGL-OPT-EXTENSION-API-20MGL-PAX-3ASECTION-29" title="(MGL-OPT:@MGL-OPT-EXTENSION-API MGL-PAX:SECTION)">Extension API</a>.</p>

<p><a name='x-28MGL-OPT-3AMINIMIZE-20FUNCTION-29'></a></p>

<ul>
<li><p>[function] <strong>MINIMIZE</strong> <em>OPTIMIZER GRADIENT-SOURCE &amp;KEY (WEIGHTS (LIST-SEGMENTS GRADIENT-SOURCE)) (DATASET *INFINITELY-EMPTY-DATASET*)</em></p>

<p>Minimize the value of the real valued function represented by
<code>GRADIENT-SOURCE</code> by updating some of its parameters in <code>WEIGHTS</code> (a <code>MAT</code>
or a sequence of MATs). Return <code>WEIGHTS</code>. <code>DATASET</code> (see
MGL:@MGL-DATASETS) is a set of unoptimized parameters of the same
function. For example, <code>WEIGHTS</code> may be the weights of a neural
network while <code>DATASET</code> is the training set consisting of inputs
suitable for MGL-TRAIN:SET-INPUT. The default <code>DATASET</code>,
(<em>EMPTY-DATASET</em>) is suitable for when all parameters are optimized,
so there is nothing left to come from the environment.</p>

<p>Optimization terminates if <code>DATASET</code> is a sampler and it runs out or
when some other condition met (see <a href="#x-28MGL-OPT-3ATERMINATION-20-28MGL-PAX-3AACCESSOR-20MGL-OPT-3AITERATIVE-OPTIMIZER-29-29" title="(MGL-OPT:TERMINATION (MGL-PAX:ACCESSOR MGL-OPT:ITERATIVE-OPTIMIZER))"><code>TERMINATION</code></a>, for example). If
<code>DATASET</code> is a <code>SEQUENCE</code>, then it is reused over and over again.</p></li>
</ul>

<p><a name='x-28MGL-OPT-3A-2AACCUMULATING-INTERESTING-GRADIENTS-2A-20VARIABLE-29'></a></p>

<ul>
<li><p>[variable] <strong>*ACCUMULATING-INTERESTING-GRADIENTS*</strong> <em>NIL</em></p>

<p>FIXME: Will go away soon.</p></li>
</ul>

<p><a name='x-28MGL-OPT-3A-40MGL-OPT-EXTENSION-API-20MGL-PAX-3ASECTION-29'></a></p>

<h3><span class="navigation"> <a href="#x-28MGL-OPT-3A-40MGL-OPT-20MGL-PAX-3ASECTION-29" title="(MGL-OPT:@MGL-OPT MGL-PAX:SECTION)">&#8592;</a> <a href="#x-28MGL-OPT-3A-40MGL-OPT-20MGL-PAX-3ASECTION-29" title="(MGL-OPT:@MGL-OPT MGL-PAX:SECTION)">&#8593;</a> <a href="#x-28MGL-OPT-3A-40MGL-OPT-OPTIMIZER-20MGL-PAX-3ASECTION-29" title="(MGL-OPT:@MGL-OPT-OPTIMIZER MGL-PAX:SECTION)">&#8594;</a> <a href="#x-28MGL-OPT-3A-40MGL-OPT-EXTENSION-API-20MGL-PAX-3ASECTION-29" title="(MGL-OPT:@MGL-OPT-EXTENSION-API MGL-PAX:SECTION)">&#8634;</a></span>6.1 Extension API</h3>

<p><a name='x-28MGL-OPT-3A-40MGL-OPT-OPTIMIZER-20MGL-PAX-3ASECTION-29'></a></p>

<h4><span class="navigation"> <a href="#x-28MGL-OPT-3A-40MGL-OPT-EXTENSION-API-20MGL-PAX-3ASECTION-29" title="(MGL-OPT:@MGL-OPT-EXTENSION-API MGL-PAX:SECTION)">&#8592;</a> <a href="#x-28MGL-OPT-3A-40MGL-OPT-EXTENSION-API-20MGL-PAX-3ASECTION-29" title="(MGL-OPT:@MGL-OPT-EXTENSION-API MGL-PAX:SECTION)">&#8593;</a> <a href="#x-28MGL-OPT-3A-40MGL-OPT-GRADIENT-SOURCE-20MGL-PAX-3ASECTION-29" title="(MGL-OPT:@MGL-OPT-GRADIENT-SOURCE MGL-PAX:SECTION)">&#8594;</a> <a href="#x-28MGL-OPT-3A-40MGL-OPT-OPTIMIZER-20MGL-PAX-3ASECTION-29" title="(MGL-OPT:@MGL-OPT-OPTIMIZER MGL-PAX:SECTION)">&#8634;</a></span>6.1.1 Implementing Optimizers</h4>

<p><a name='x-28MGL-OPT-3AMINIMIZE-2A-20GENERIC-FUNCTION-29'></a></p>

<ul>
<li>[generic-function] <strong>MINIMIZE*</strong> <em>OPTIMIZER GRADIENT-SOURCE WEIGHTS DATASET</em></li>
</ul>

<p><a name='x-28MGL-OPT-3AINITIALIZE-OPTIMIZER-2A-20GENERIC-FUNCTION-29'></a></p>

<ul>
<li><p>[generic-function] <strong>INITIALIZE-OPTIMIZER*</strong> <em>OPTIMIZER GRADIENT-SOURCE WEIGHTS DATASET</em></p>

<p>Called automatically before training starts, this
function sets up <code>OPTIMIZER</code> to be suitable for optimizing
<code>GRADIENT-SOURCE</code>. It typically creates appropriately sized
accumulators for the gradients.</p></li>
</ul>

<p><a name='x-28MGL-OPT-3ATERMINATE-OPTIMIZATION-P-20FUNCTION-29'></a></p>

<ul>
<li><p>[function] <strong>TERMINATE-OPTIMIZATION-P</strong> <em>N-INSTANCES TERMINATION</em></p>

<p>Utility function for subclasses of <a href="#x-28MGL-OPT-3AITERATIVE-OPTIMIZER-20CLASS-29" title="(MGL-OPT:ITERATIVE-OPTIMIZER CLASS)"><code>ITERATIVE-OPTIMIZER</code></a>. It returns
whether optimization is to be terminated based on <code>N-INSTANCES</code> and
<code>TERMINATION</code> that are values of the respective accessors of
<a href="#x-28MGL-OPT-3AITERATIVE-OPTIMIZER-20CLASS-29" title="(MGL-OPT:ITERATIVE-OPTIMIZER CLASS)"><code>ITERATIVE-OPTIMIZER</code></a>.</p></li>
</ul>

<p><a name='x-28MGL-OPT-3ASEGMENT-SET-20CLASS-29'></a></p>

<ul>
<li><p>[class] <strong>SEGMENT-SET</strong></p>

<p>It's like a concatenation of segments.</p></li>
</ul>

<p><a name='x-28MGL-COMMON-3ASIZE-20-28MGL-PAX-3AREADER-20MGL-OPT-3ASEGMENT-SET-29-29'></a></p>

<ul>
<li><p>[reader] <strong>SIZE</strong> <em>SEGMENT-SET</em></p>

<p>The sum of the sizes of the weight matrices of
<code>SEGMENTS</code>.</p></li>
</ul>

<p><a name='x-28MGL-OPT-3ADO-SEGMENT-SET-20MGL-PAX-3AMACRO-29'></a></p>

<ul>
<li><p>[macro] <strong>DO-SEGMENT-SET</strong> <em>(SEGMENT &amp;KEY START-IN-SEGMENT-SET) SEGMENT-SET &amp;BODY BODY</em></p>

<p>Iterate over <code>SEGMENTS</code> in <code>SEGMENT-SET</code> ....</p></li>
</ul>

<p><a name='x-28MGL-OPT-3ASEGMENT-SET-3C-MAT-20FUNCTION-29'></a></p>

<ul>
<li><p>[function] <strong>SEGMENT-SET&lt;-MAT</strong> <em>SEGMENT-SET MAT</em></p>

<p>Copy the values of <code>MAT</code> to <code>SEGMENT-SET</code>.</p></li>
</ul>

<p><a name='x-28MGL-OPT-3ASEGMENT-SET--3EMAT-20FUNCTION-29'></a></p>

<ul>
<li><p>[function] <strong>SEGMENT-SET-&gt;MAT</strong> <em>SEGMENT-SET MAT</em></p>

<p>Copy the values of <code>SEGMENT-SET</code> to <code>MAT</code>.</p></li>
</ul>

<p><a name='x-28MGL-OPT-3A-40MGL-OPT-GRADIENT-SOURCE-20MGL-PAX-3ASECTION-29'></a></p>

<h4><span class="navigation"> <a href="#x-28MGL-OPT-3A-40MGL-OPT-OPTIMIZER-20MGL-PAX-3ASECTION-29" title="(MGL-OPT:@MGL-OPT-OPTIMIZER MGL-PAX:SECTION)">&#8592;</a> <a href="#x-28MGL-OPT-3A-40MGL-OPT-EXTENSION-API-20MGL-PAX-3ASECTION-29" title="(MGL-OPT:@MGL-OPT-EXTENSION-API MGL-PAX:SECTION)">&#8593;</a> <a href="#x-28MGL-OPT-3A-40MGL-OPT-GRADIENT-SINK-20MGL-PAX-3ASECTION-29" title="(MGL-OPT:@MGL-OPT-GRADIENT-SINK MGL-PAX:SECTION)">&#8594;</a> <a href="#x-28MGL-OPT-3A-40MGL-OPT-GRADIENT-SOURCE-20MGL-PAX-3ASECTION-29" title="(MGL-OPT:@MGL-OPT-GRADIENT-SOURCE MGL-PAX:SECTION)">&#8634;</a></span>6.1.2 Implementing Gradient Sources</h4>

<p>Weights can be stored in a multitude of ways. It is assumed that
weights are stored in any number of <code>MAT</code> objects.</p>

<p><a name='x-28MGL-OPT-3AMAP-SEGMENTS-20GENERIC-FUNCTION-29'></a></p>

<ul>
<li><p>[generic-function] <strong>MAP-SEGMENTS</strong> <em>FN GRADIENT-SOURCE</em></p>

<p>Apply <code>FN</code> to each segment of <code>GRADIENT-SOURCE</code>.</p></li>
</ul>

<p><a name='x-28MGL-OPT-3ALIST-SEGMENTS-20FUNCTION-29'></a></p>

<ul>
<li><p>[function] <strong>LIST-SEGMENTS</strong> <em>GRADIENT-SOURCE</em></p>

<p>Return the list of segments from <a href="#x-28MGL-OPT-3AMAP-SEGMENTS-20GENERIC-FUNCTION-29" title="(MGL-OPT:MAP-SEGMENTS GENERIC-FUNCTION)"><code>MAP-SEGMENTS</code></a> on <code>GRADIENT-SOURCE</code>.</p></li>
</ul>

<p><a name='x-28MGL-OPT-3AINITIALIZE-GRADIENT-SOURCE-2A-20GENERIC-FUNCTION-29'></a></p>

<ul>
<li><p>[generic-function] <strong>INITIALIZE-GRADIENT-SOURCE*</strong> <em>OPTIMIZER GRADIENT-SOURCE WEIGHTS DATASET</em></p>

<p>Called automatically before training starts, this
function sets up <code>SINK</code> to be suitable for <code>SOURCE</code>. It typically
creates accumulator arrays in the sink for the gradients.</p></li>
</ul>

<p><a name='x-28MGL-OPT-3AACCUMULATE-GRADIENTS-2A-20GENERIC-FUNCTION-29'></a></p>

<ul>
<li><p>[generic-function] <strong>ACCUMULATE-GRADIENTS*</strong> <em>SOURCE SINK BATCH MULTIPLIER VALUEP</em></p>

<p>Add <code>MULTIPLIER</code> times the sum of first-order
gradients to accumulators of <code>SINK</code> (normally accessed with
<a href="#x-28MGL-OPT-3ADO-GRADIENT-SINK-20MGL-PAX-3AMACRO-29" title="(MGL-OPT:DO-GRADIENT-SINK MGL-PAX:MACRO)"><code>DO-GRADIENT-SINK</code></a>) and if <code>VALUEP</code>, return the sum of values of the
function being optimized for a <code>BATCH</code> of instances. <code>SOURCE</code> is the
object representing the function being optimized, <code>SINK</code> is gradient
sink.</p></li>
</ul>

<p><a name='x-28MGL-OPT-3AMAP-SEGMENTS-20GENERIC-FUNCTION-29'></a></p>

<ul>
<li><p>[generic-function] <strong>MAP-SEGMENTS</strong> <em>FN GRADIENT-SOURCE</em></p>

<p>Apply <code>FN</code> to each segment of <code>GRADIENT-SOURCE</code>.</p></li>
</ul>

<p><a name='x-28MGL-OPT-3AMAP-SEGMENT-RUNS-20GENERIC-FUNCTION-29'></a></p>

<ul>
<li><p>[generic-function] <strong>MAP-SEGMENT-RUNS</strong> <em>FN SEGMENT</em></p>

<p>Call <code>FN</code> with start and end of intervals of
consecutive indices that are not missing in <code>SEGMENT</code>. Called by
optimizers that support partial updates.</p></li>
</ul>

<p><a name='x-28MGL-OPT-3ASEGMENT-WEIGHTS-20GENERIC-FUNCTION-29'></a></p>

<ul>
<li><p>[generic-function] <strong>SEGMENT-WEIGHTS</strong> <em>SEGMENT</em></p>

<p>Return the weight matrix of <code>SEGMENT</code>.</p></li>
</ul>

<p><a name='x-28MGL-OPT-3A-40MGL-OPT-GRADIENT-SINK-20MGL-PAX-3ASECTION-29'></a></p>

<h4><span class="navigation"> <a href="#x-28MGL-OPT-3A-40MGL-OPT-GRADIENT-SOURCE-20MGL-PAX-3ASECTION-29" title="(MGL-OPT:@MGL-OPT-GRADIENT-SOURCE MGL-PAX:SECTION)">&#8592;</a> <a href="#x-28MGL-OPT-3A-40MGL-OPT-EXTENSION-API-20MGL-PAX-3ASECTION-29" title="(MGL-OPT:@MGL-OPT-EXTENSION-API MGL-PAX:SECTION)">&#8593;</a> <a href="#x-28MGL-OPT-3A-40MGL-OPT-ITERATIVE-OPTIMIZER-20MGL-PAX-3ASECTION-29" title="(MGL-OPT:@MGL-OPT-ITERATIVE-OPTIMIZER MGL-PAX:SECTION)">&#8594;</a> <a href="#x-28MGL-OPT-3A-40MGL-OPT-GRADIENT-SINK-20MGL-PAX-3ASECTION-29" title="(MGL-OPT:@MGL-OPT-GRADIENT-SINK MGL-PAX:SECTION)">&#8634;</a></span>6.1.3 Implementing Gradient Sinks</h4>

<p><a name='x-28MGL-OPT-3AMAP-GRADIENT-SINK-20GENERIC-FUNCTION-29'></a></p>

<ul>
<li><p>[generic-function] <strong>MAP-GRADIENT-SINK</strong> <em>FN SINK</em></p>

<p>Call <code>FN</code> of lambda list (<code>SEGMENT</code> <code>ACCUMULATOR</code>) on
each segment and their corresponding accumulator <code>MAT</code> in <code>SINK</code>.</p></li>
</ul>

<p><a name='x-28MGL-OPT-3ADO-GRADIENT-SINK-20MGL-PAX-3AMACRO-29'></a></p>

<ul>
<li>[macro] <strong>DO-GRADIENT-SINK</strong> <em>((SEGMENT ACCUMULATOR) SINK) &amp;BODY BODY</em></li>
</ul>

<p><a name='x-28MGL-OPT-3ACALL-WITH-SINK-ACCUMULATOR-20GENERIC-FUNCTION-29'></a></p>

<ul>
<li>[generic-function] <strong>CALL-WITH-SINK-ACCUMULATOR</strong> <em>FN SEGMENT SOURCE SINK</em></li>
</ul>

<p><a name='x-28MGL-OPT-3AWITH-SINK-ACCUMULATOR-20MGL-PAX-3AMACRO-29'></a></p>

<ul>
<li>[macro] <strong>WITH-SINK-ACCUMULATOR</strong> <em>(ACCUMULATOR (SEGMENT SOURCE SINK)) &amp;BODY BODY</em></li>
</ul>

<p><a name='x-28MGL-OPT-3AACCUMULATED-IN-SINK-P-20FUNCTION-29'></a></p>

<ul>
<li>[function] <strong>ACCUMULATED-IN-SINK-P</strong> <em>SEGMENT SOURCE SINK</em></li>
</ul>

<p><a name='x-28MGL-OPT-3A-40MGL-OPT-ITERATIVE-OPTIMIZER-20MGL-PAX-3ASECTION-29'></a></p>

<h3><span class="navigation"> <a href="#x-28MGL-OPT-3A-40MGL-OPT-GRADIENT-SINK-20MGL-PAX-3ASECTION-29" title="(MGL-OPT:@MGL-OPT-GRADIENT-SINK MGL-PAX:SECTION)">&#8592;</a> <a href="#x-28MGL-OPT-3A-40MGL-OPT-20MGL-PAX-3ASECTION-29" title="(MGL-OPT:@MGL-OPT MGL-PAX:SECTION)">&#8593;</a> <a href="#x-28MGL-GD-3A-40MGL-GD-20MGL-PAX-3ASECTION-29" title="(MGL-GD:@MGL-GD MGL-PAX:SECTION)">&#8594;</a> <a href="#x-28MGL-OPT-3A-40MGL-OPT-ITERATIVE-OPTIMIZER-20MGL-PAX-3ASECTION-29" title="(MGL-OPT:@MGL-OPT-ITERATIVE-OPTIMIZER MGL-PAX:SECTION)">&#8634;</a></span>6.2 Iterative Optimizer</h3>

<p><a name='x-28MGL-OPT-3AITERATIVE-OPTIMIZER-20CLASS-29'></a></p>

<ul>
<li><p>[class] <strong>ITERATIVE-OPTIMIZER</strong></p>

<p>An abstract base class of <a href="#x-28MGL-GD-3A-40MGL-GD-20MGL-PAX-3ASECTION-29" title="(MGL-GD:@MGL-GD MGL-PAX:SECTION)">Gradient Descent</a> and
<a href="#x-28MGL-CG-3A-40MGL-CG-20MGL-PAX-3ASECTION-29" title="(MGL-CG:@MGL-CG MGL-PAX:SECTION)">Conjugate Gradient</a> based optimizers that iterate over instances until a
termination condition is met.</p></li>
</ul>

<p><a name='x-28MGL-OPT-3AN-INSTANCES-20-28MGL-PAX-3AREADER-20MGL-OPT-3AITERATIVE-OPTIMIZER-29-29'></a></p>

<ul>
<li><p>[reader] <strong>N-INSTANCES</strong> <em>ITERATIVE-OPTIMIZER</em> <em>(:N-INSTANCES = 0)</em></p>

<p>The number of instances this optimizer has seen so
far. Incremented automatically during optimization.</p></li>
</ul>

<p><a name='x-28MGL-OPT-3ATERMINATION-20-28MGL-PAX-3AACCESSOR-20MGL-OPT-3AITERATIVE-OPTIMIZER-29-29'></a></p>

<ul>
<li><p>[accessor] <strong>TERMINATION</strong> <em>ITERATIVE-OPTIMIZER</em> <em>(:TERMINATION = NIL)</em></p>

<p>If a number, it's the number of instances to train
on in the sense of <a href="#x-28MGL-OPT-3AN-INSTANCES-20-28MGL-PAX-3AREADER-20MGL-OPT-3AITERATIVE-OPTIMIZER-29-29" title="(MGL-OPT:N-INSTANCES (MGL-PAX:READER MGL-OPT:ITERATIVE-OPTIMIZER))"><code>N-INSTANCES</code></a>. If <a href="#x-28MGL-OPT-3AN-INSTANCES-20-28MGL-PAX-3AREADER-20MGL-OPT-3AITERATIVE-OPTIMIZER-29-29" title="(MGL-OPT:N-INSTANCES (MGL-PAX:READER MGL-OPT:ITERATIVE-OPTIMIZER))"><code>N-INSTANCES</code></a> is equal or greater
than this value optimization stops. If <a href="#x-28MGL-OPT-3ATERMINATION-20-28MGL-PAX-3AACCESSOR-20MGL-OPT-3AITERATIVE-OPTIMIZER-29-29" title="(MGL-OPT:TERMINATION (MGL-PAX:ACCESSOR MGL-OPT:ITERATIVE-OPTIMIZER))"><code>TERMINATION</code></a> is <code>NIL</code>, then
optimization will continue. If it is <code>T</code>, then optimization will
stop. If it is a function of no arguments, then its return value
is processed as if it was returned by <a href="#x-28MGL-OPT-3ATERMINATION-20-28MGL-PAX-3AACCESSOR-20MGL-OPT-3AITERATIVE-OPTIMIZER-29-29" title="(MGL-OPT:TERMINATION (MGL-PAX:ACCESSOR MGL-OPT:ITERATIVE-OPTIMIZER))"><code>TERMINATION</code></a>.</p></li>
</ul>

<p><a name='x-28MGL-OPT-3ASET-N-INSTANCES-20GENERIC-FUNCTION-29'></a></p>

<ul>
<li><p>[generic-function] <strong>SET-N-INSTANCES</strong> <em>OPTIMIZER GRADIENT-SOURCE N-INSTANCES</em></p>

<p>Called whenever <code>N-INSTANCES</code> of <code>OPTIMIZER</code> is
incremented. Hang an <code>:AFTER</code> method on this to print some
statistics.</p></li>
</ul>

<p><a name='x-28MGL-GD-3A-40MGL-GD-20MGL-PAX-3ASECTION-29'></a></p>

<h3><span class="navigation"> <a href="#x-28MGL-OPT-3A-40MGL-OPT-ITERATIVE-OPTIMIZER-20MGL-PAX-3ASECTION-29" title="(MGL-OPT:@MGL-OPT-ITERATIVE-OPTIMIZER MGL-PAX:SECTION)">&#8592;</a> <a href="#x-28MGL-OPT-3A-40MGL-OPT-20MGL-PAX-3ASECTION-29" title="(MGL-OPT:@MGL-OPT MGL-PAX:SECTION)">&#8593;</a> <a href="#x-28MGL-GD-3A-40MGL-GD-BATCH-GD-OPTIMIZER-20MGL-PAX-3ASECTION-29" title="(MGL-GD:@MGL-GD-BATCH-GD-OPTIMIZER MGL-PAX:SECTION)">&#8594;</a> <a href="#x-28MGL-GD-3A-40MGL-GD-20MGL-PAX-3ASECTION-29" title="(MGL-GD:@MGL-GD MGL-PAX:SECTION)">&#8634;</a></span>6.3 Gradient Descent</h3>

<h6>[in package MGL-GD]</h6>

<p>Gradient descent is a first-order optimization algorithm. Relying
completely on first derivatives, it does not even evaluate the
function to be minimized. Let's see how to minimize a numerical lisp
function with respect to some of its parameters.</p>

<pre><code><span class="code"><span class="comment">;;; Create an object representing the sine function.
</span><span class="paren1">(<span class="code"><i><span class="symbol">defparameter</span></i> <span class="special">*diff-fn-1*</span>
  <span class="paren2">(<span class="code">make-instance 'mgl-diffun:diffun
                 <span class="keyword">:fn</span> #'sin
                 <span class="comment">;; We are going to optimize its only parameter.
</span>                 <span class="keyword">:weight-indices</span> '<span class="paren3">(<span class="code">0</span>)</span></span>)</span></span>)</span>

<span class="comment">;;; Minimize SIN. Note that there is no dataset involved because all
</span><span class="comment">;;; parameters are being optimized.
</span><span class="paren1">(<span class="code">minimize <span class="paren2">(<span class="code">make-instance 'batch-gd-optimizer <span class="keyword">:termination</span> 1000</span>)</span>
          <span class="special">*diff-fn-1*</span>
          <span class="keyword">:weights</span> <span class="paren2">(<span class="code">make-mat 1</span>)</span></span>)</span>
<span class="comment">;;; =&gt; A MAT with a single value of about -pi/2.
</span>
<span class="comment">;;; Create a differentiable function for f(x,y)=(x-y)^2. X is a
</span><span class="comment">;;; parameter whose values come from the DATASET argument passed to
</span><span class="comment">;;; MINIMIZE. Y is a parameter to be optimized (a 'weight').
</span><span class="paren1">(<span class="code"><i><span class="symbol">defparameter</span></i> <span class="special">*diff-fn-2*</span>
  <span class="paren2">(<span class="code">make-instance 'mgl-diffun:diffun
                 <span class="keyword">:fn</span> <span class="paren3">(<span class="code"><i><span class="symbol">lambda</span></i> <span class="paren4">(<span class="code">x y</span>)</span>
                       <span class="paren4">(<span class="code">expt <span class="paren5">(<span class="code">- x y</span>)</span> 2</span>)</span></span>)</span>
                 <span class="keyword">:parameter-indices</span> '<span class="paren3">(<span class="code">0</span>)</span>
                 <span class="keyword">:weight-indices</span> '<span class="paren3">(<span class="code">1</span>)</span></span>)</span></span>)</span>

<span class="comment">;;; Find the Y that minimizes the distance from the instances
</span><span class="comment">;;; generated by the sampler.
</span><span class="paren1">(<span class="code">minimize <span class="paren2">(<span class="code">make-instance 'batch-gd-optimizer <span class="keyword">:batch-size</span> 10</span>)</span>
          <span class="special">*diff-fn-2*</span>
          <span class="keyword">:weights</span> <span class="paren2">(<span class="code">make-mat 1</span>)</span>
          <span class="keyword">:dataset</span> <span class="paren2">(<span class="code">make-instance 'function-sampler
                                  <span class="keyword">:sampler</span> <span class="paren3">(<span class="code"><i><span class="symbol">lambda</span></i> <span class="paren4">(<span class="code"></span>)</span>
                                             <span class="paren4">(<span class="code">list <span class="paren5">(<span class="code">+ 10
                                                      <span class="paren6">(<span class="code">gaussian-random-1</span>)</span></span>)</span></span>)</span></span>)</span>
                                  <span class="keyword">:max-n-samples</span> 1000</span>)</span></span>)</span>
<span class="comment">;;; =&gt; A MAT with a single value of about 10, the expected value of
</span><span class="comment">;;; the instances in the dataset.
</span>
<span class="comment">;;; The dataset can be a SEQUENCE in which case we'd better set
</span><span class="comment">;;; TERMINATION else optimization would never finish.
</span><span class="paren1">(<span class="code">minimize <span class="paren2">(<span class="code">make-instance 'batch-gd-optimizer
                         <span class="keyword">:termination</span> 1000</span>)</span>
          <span class="special">*diff-fn-2*</span>
          <span class="keyword">:weights</span> <span class="paren2">(<span class="code">make-mat 1</span>)</span>
          <span class="keyword">:dataset</span> '<span class="paren2">(<span class="code"><span class="paren3">(<span class="code">0</span>)</span> <span class="paren3">(<span class="code">1</span>)</span> <span class="paren3">(<span class="code">2</span>)</span> <span class="paren3">(<span class="code">3</span>)</span> <span class="paren3">(<span class="code">4</span>)</span> <span class="paren3">(<span class="code">5</span>)</span></span>)</span></span>)</span>
<span class="comment">;;; =&gt; A MAT with a single value of about 2.5.</span></span></code></pre>

<p>We are going to see a number of accessors for optimizer paramaters.
In general, it's allowed to <code>SETF</code> real slot accessors (as opposed to
readers and writers) at any time during optimization and so is
defining a method on an optimizer subclass that computes the value
in any way. For example, to decay the learning rate on a per
mini-batch basis:</p>

<pre><code>(defmethod learning-rate ((optimizer my-batch-gd-optimizer))
  (* (slot-value optimizer 'learning-rate)
     (expt 0.998
           (/ (n-instances optimizer) 60000))))
</code></pre>

<p><a name='x-28MGL-GD-3A-40MGL-GD-BATCH-GD-OPTIMIZER-20MGL-PAX-3ASECTION-29'></a></p>

<h4><span class="navigation"> <a href="#x-28MGL-GD-3A-40MGL-GD-20MGL-PAX-3ASECTION-29" title="(MGL-GD:@MGL-GD MGL-PAX:SECTION)">&#8592;</a> <a href="#x-28MGL-GD-3A-40MGL-GD-20MGL-PAX-3ASECTION-29" title="(MGL-GD:@MGL-GD MGL-PAX:SECTION)">&#8593;</a> <a href="#x-28MGL-GD-3A-40MGL-GD-SEGMENTED-GD-OPTIMIZER-20MGL-PAX-3ASECTION-29" title="(MGL-GD:@MGL-GD-SEGMENTED-GD-OPTIMIZER MGL-PAX:SECTION)">&#8594;</a> <a href="#x-28MGL-GD-3A-40MGL-GD-BATCH-GD-OPTIMIZER-20MGL-PAX-3ASECTION-29" title="(MGL-GD:@MGL-GD-BATCH-GD-OPTIMIZER MGL-PAX:SECTION)">&#8634;</a></span>6.3.1 Batch GD Optimizer</h4>

<p><a name='x-28MGL-GD-3ABATCH-GD-OPTIMIZER-20CLASS-29'></a></p>

<ul>
<li><p>[class] <strong>BATCH-GD-OPTIMIZER</strong> <em>GD-OPTIMIZER</em></p>

<p>Updates all weights simultaneously after chewing
through <code>BATCH-SIZE</code>(<a href="#x-28MGL-COMMON-3ABATCH-SIZE-20-28MGL-PAX-3AACCESSOR-20MGL-CG-3ACG-OPTIMIZER-29-29" title="(MGL-COMMON:BATCH-SIZE (MGL-PAX:ACCESSOR MGL-CG:CG-OPTIMIZER))"><code>0</code></a> <a href="#x-28MGL-COMMON-3ABATCH-SIZE-20-28MGL-PAX-3AACCESSOR-20MGL-GD-3A-3AGD-OPTIMIZER-29-29" title="(MGL-COMMON:BATCH-SIZE (MGL-PAX:ACCESSOR MGL-GD::GD-OPTIMIZER))"><code>1</code></a>) inputs. <a href="#x-28MGL-GD-3APER-WEIGHT-BATCH-GD-OPTIMIZER-20CLASS-29" title="(MGL-GD:PER-WEIGHT-BATCH-GD-OPTIMIZER CLASS)"><code>PER-WEIGHT-BATCH-GD-OPTIMIZER</code></a> may be a
better choice when some weights can go unused for instance due to
missing input values.</p>

<p>Assuming that <code>ACCUMULATOR</code> has the sum of gradients for a mini-batch,
the weight update looks like this:</p>

<pre><code>delta_w' += momentum * delta_w +
            accumulator / batch_size + l2 * w + l1 * sign(w)

w' -= learning_rate * delta_w'
</code></pre>

<p>which is the same as the more traditional formulation:</p>

<pre><code>delta_w' += momentum * delta_w +
            learning_rate * (df/dw / batch_size + l2 * w + l1 * sign(w))

w' -= delta_w'
</code></pre>

<p>but the former works better when batch size, momentum or learning
rate change during the course of optimization. The above is with
normal momentum, Nesterov's momentum (see <a href="#x-28MGL-GD-3AMOMENTUM-TYPE-20-28MGL-PAX-3AREADER-20MGL-GD-3A-3AGD-OPTIMIZER-29-29" title="(MGL-GD:MOMENTUM-TYPE (MGL-PAX:READER MGL-GD::GD-OPTIMIZER))"><code>MOMENTUM-TYPE</code></a>) momentum is
also available.</p></li>
</ul>

<p><a name='x-28MGL-OPT-3AN-INSTANCES-20-28MGL-PAX-3AREADER-20MGL-OPT-3AITERATIVE-OPTIMIZER-29-29'></a></p>

<ul>
<li><p>[reader] <strong>N-INSTANCES</strong> <em>ITERATIVE-OPTIMIZER</em> <em>(:N-INSTANCES = 0)</em></p>

<p>The number of instances this optimizer has seen so
far. Incremented automatically during optimization.</p></li>
</ul>

<p><a name='x-28MGL-OPT-3ATERMINATION-20-28MGL-PAX-3AACCESSOR-20MGL-OPT-3AITERATIVE-OPTIMIZER-29-29'></a></p>

<ul>
<li><p>[accessor] <strong>TERMINATION</strong> <em>ITERATIVE-OPTIMIZER</em> <em>(:TERMINATION = NIL)</em></p>

<p>If a number, it's the number of instances to train
on in the sense of <a href="#x-28MGL-OPT-3AN-INSTANCES-20-28MGL-PAX-3AREADER-20MGL-OPT-3AITERATIVE-OPTIMIZER-29-29" title="(MGL-OPT:N-INSTANCES (MGL-PAX:READER MGL-OPT:ITERATIVE-OPTIMIZER))"><code>N-INSTANCES</code></a>. If <a href="#x-28MGL-OPT-3AN-INSTANCES-20-28MGL-PAX-3AREADER-20MGL-OPT-3AITERATIVE-OPTIMIZER-29-29" title="(MGL-OPT:N-INSTANCES (MGL-PAX:READER MGL-OPT:ITERATIVE-OPTIMIZER))"><code>N-INSTANCES</code></a> is equal or greater
than this value optimization stops. If <a href="#x-28MGL-OPT-3ATERMINATION-20-28MGL-PAX-3AACCESSOR-20MGL-OPT-3AITERATIVE-OPTIMIZER-29-29" title="(MGL-OPT:TERMINATION (MGL-PAX:ACCESSOR MGL-OPT:ITERATIVE-OPTIMIZER))"><code>TERMINATION</code></a> is <code>NIL</code>, then
optimization will continue. If it is <code>T</code>, then optimization will
stop. If it is a function of no arguments, then its return value
is processed as if it was returned by <a href="#x-28MGL-OPT-3ATERMINATION-20-28MGL-PAX-3AACCESSOR-20MGL-OPT-3AITERATIVE-OPTIMIZER-29-29" title="(MGL-OPT:TERMINATION (MGL-PAX:ACCESSOR MGL-OPT:ITERATIVE-OPTIMIZER))"><code>TERMINATION</code></a>.</p></li>
</ul>

<p><a name='x-28MGL-COMMON-3ABATCH-SIZE-20-28MGL-PAX-3AACCESSOR-20MGL-GD-3A-3AGD-OPTIMIZER-29-29'></a></p>

<ul>
<li><p>[accessor] <strong>BATCH-SIZE</strong> <em>GD-OPTIMIZER</em> <em>(:BATCH-SIZE = 1)</em></p>

<p>After having gone through <code>BATCH-SIZE</code>(<a href="#x-28MGL-COMMON-3ABATCH-SIZE-20-28MGL-PAX-3AACCESSOR-20MGL-CG-3ACG-OPTIMIZER-29-29" title="(MGL-COMMON:BATCH-SIZE (MGL-PAX:ACCESSOR MGL-CG:CG-OPTIMIZER))"><code>0</code></a> <a href="#x-28MGL-COMMON-3ABATCH-SIZE-20-28MGL-PAX-3AACCESSOR-20MGL-GD-3A-3AGD-OPTIMIZER-29-29" title="(MGL-COMMON:BATCH-SIZE (MGL-PAX:ACCESSOR MGL-GD::GD-OPTIMIZER))"><code>1</code></a>) number of
inputs, weights are updated. With <code>BATCH-SIZE</code>(<a href="#x-28MGL-COMMON-3ABATCH-SIZE-20-28MGL-PAX-3AACCESSOR-20MGL-CG-3ACG-OPTIMIZER-29-29" title="(MGL-COMMON:BATCH-SIZE (MGL-PAX:ACCESSOR MGL-CG:CG-OPTIMIZER))"><code>0</code></a> <a href="#x-28MGL-COMMON-3ABATCH-SIZE-20-28MGL-PAX-3AACCESSOR-20MGL-GD-3A-3AGD-OPTIMIZER-29-29" title="(MGL-COMMON:BATCH-SIZE (MGL-PAX:ACCESSOR MGL-GD::GD-OPTIMIZER))"><code>1</code></a>) 1, one gets
Stochastics Gradient Descent. With <code>BATCH-SIZE</code>(<a href="#x-28MGL-COMMON-3ABATCH-SIZE-20-28MGL-PAX-3AACCESSOR-20MGL-CG-3ACG-OPTIMIZER-29-29" title="(MGL-COMMON:BATCH-SIZE (MGL-PAX:ACCESSOR MGL-CG:CG-OPTIMIZER))"><code>0</code></a> <a href="#x-28MGL-COMMON-3ABATCH-SIZE-20-28MGL-PAX-3AACCESSOR-20MGL-GD-3A-3AGD-OPTIMIZER-29-29" title="(MGL-COMMON:BATCH-SIZE (MGL-PAX:ACCESSOR MGL-GD::GD-OPTIMIZER))"><code>1</code></a>) equal to the number
of instances in the dataset, one gets standard, 'batch' gradient
descent. With <code>BATCH-SIZE</code>(<a href="#x-28MGL-COMMON-3ABATCH-SIZE-20-28MGL-PAX-3AACCESSOR-20MGL-CG-3ACG-OPTIMIZER-29-29" title="(MGL-COMMON:BATCH-SIZE (MGL-PAX:ACCESSOR MGL-CG:CG-OPTIMIZER))"><code>0</code></a> <a href="#x-28MGL-COMMON-3ABATCH-SIZE-20-28MGL-PAX-3AACCESSOR-20MGL-GD-3A-3AGD-OPTIMIZER-29-29" title="(MGL-COMMON:BATCH-SIZE (MGL-PAX:ACCESSOR MGL-GD::GD-OPTIMIZER))"><code>1</code></a>) between these two extremes, one gets the
most practical 'mini-batch' compromise.</p></li>
</ul>

<p><a name='x-28MGL-GD-3ALEARNING-RATE-20-28MGL-PAX-3AACCESSOR-20MGL-GD-3A-3AGD-OPTIMIZER-29-29'></a></p>

<ul>
<li><p>[accessor] <strong>LEARNING-RATE</strong> <em>GD-OPTIMIZER</em> <em>(:LEARNING-RATE = 0.10000000149011612d0)</em></p>

<p>This is the step size along the gradient. Decrease
it if optimization diverges, increase it if it doesn't make
progress.</p></li>
</ul>

<p><a name='x-28MGL-GD-3AMOMENTUM-20-28MGL-PAX-3AACCESSOR-20MGL-GD-3A-3AGD-OPTIMIZER-29-29'></a></p>

<ul>
<li><p>[accessor] <strong>MOMENTUM</strong> <em>GD-OPTIMIZER</em> <em>(:MOMENTUM = 0.0d0)</em></p>

<p>A value in the [0, 1) interval. <a href="#x-28MGL-GD-3AMOMENTUM-20-28MGL-PAX-3AACCESSOR-20MGL-GD-3A-3AGD-OPTIMIZER-29-29" title="(MGL-GD:MOMENTUM (MGL-PAX:ACCESSOR MGL-GD::GD-OPTIMIZER))"><code>MOMENTUM</code></a> times the
previous weight change is added to the gradient. 0 means no
momentum.</p></li>
</ul>

<p><a name='x-28MGL-GD-3AMOMENTUM-TYPE-20-28MGL-PAX-3AREADER-20MGL-GD-3A-3AGD-OPTIMIZER-29-29'></a></p>

<ul>
<li><p>[reader] <strong>MOMENTUM-TYPE</strong> <em>GD-OPTIMIZER</em> <em>(:MOMENTUM-TYPE = :NORMAL)</em></p>

<p>One of <code>:NORMAL</code> and <code>:NESTEROV</code>. For pure
optimization Nesterov's momentum may be better, but it also
increases chances of overfitting.</p></li>
</ul>

<p><a name='x-28MGL-GD-3AWEIGHT-DECAY-20-28MGL-PAX-3AACCESSOR-20MGL-GD-3A-3AGD-OPTIMIZER-29-29'></a></p>

<ul>
<li><p>[accessor] <strong>WEIGHT-DECAY</strong> <em>GD-OPTIMIZER</em> <em>(:WEIGHT-DECAY = 0.0d0)</em></p>

<p>An L2 penalty. It discourages large weights, much
like a zero mean gaussian prior. <a href="#x-28MGL-GD-3AWEIGHT-DECAY-20-28MGL-PAX-3AACCESSOR-20MGL-GD-3A-3AGD-OPTIMIZER-29-29" title="(MGL-GD:WEIGHT-DECAY (MGL-PAX:ACCESSOR MGL-GD::GD-OPTIMIZER))"><code>WEIGHT-DECAY</code></a> * WEIGHT is added to
the gradient to penalize large weights. It's as if the function
whose minimum is sought had WEIGHT-DECAY*sum_i{0.5 * WEIGHT_i^2}
added to it.</p></li>
</ul>

<p><a name='x-28MGL-GD-3AWEIGHT-PENALTY-20-28MGL-PAX-3AACCESSOR-20MGL-GD-3A-3AGD-OPTIMIZER-29-29'></a></p>

<ul>
<li><p>[accessor] <strong>WEIGHT-PENALTY</strong> <em>GD-OPTIMIZER</em> <em>(:WEIGHT-PENALTY = 0.0d0)</em></p>

<p>An L1 penalty. It encourages sparsity.
<code>SIGN</code>(WEIGHT) * <a href="#x-28MGL-GD-3AWEIGHT-PENALTY-20-28MGL-PAX-3AACCESSOR-20MGL-GD-3A-3AGD-OPTIMIZER-29-29" title="(MGL-GD:WEIGHT-PENALTY (MGL-PAX:ACCESSOR MGL-GD::GD-OPTIMIZER))"><code>WEIGHT-PENALTY</code></a> is added to the gradient pushing the
weight towards negative infinity. It's as if the function whose
minima is sought had WEIGHT-PENALTY*sum_i{abs(WEIGHT_i)} added to
it. Putting it on feature biases consitutes a sparsity constraint
on the features.</p></li>
</ul>

<p><a name='x-28MGL-GD-3AAFTER-UPDATE-HOOK-20-28MGL-PAX-3AACCESSOR-20MGL-GD-3A-3AGD-OPTIMIZER-29-29'></a></p>

<ul>
<li><p>[accessor] <strong>AFTER-UPDATE-HOOK</strong> <em>GD-OPTIMIZER</em> <em>(:AFTER-UPDATE-HOOK = NIL)</em></p>

<p>A list of functions with no arguments called after
each weight update.</p></li>
</ul>

<p><a name='x-28MGL-GD-3ABEFORE-UPDATE-HOOK-20-28MGL-PAX-3AACCESSOR-20MGL-GD-3ABATCH-GD-OPTIMIZER-29-29'></a></p>

<ul>
<li><p>[accessor] <strong>BEFORE-UPDATE-HOOK</strong> <em>BATCH-GD-OPTIMIZER</em> <em>(:BEFORE-UPDATE-HOOK = NIL)</em></p>

<p>A list of functions of no parameters. Each
function is called just before a weight update takes place.
Convenient to hang some additional gradient accumulating code
on.</p></li>
</ul>

<p><a name='x-28MGL-GD-3A-40MGL-GD-SEGMENTED-GD-OPTIMIZER-20MGL-PAX-3ASECTION-29'></a></p>

<h4><span class="navigation"> <a href="#x-28MGL-GD-3A-40MGL-GD-BATCH-GD-OPTIMIZER-20MGL-PAX-3ASECTION-29" title="(MGL-GD:@MGL-GD-BATCH-GD-OPTIMIZER MGL-PAX:SECTION)">&#8592;</a> <a href="#x-28MGL-GD-3A-40MGL-GD-20MGL-PAX-3ASECTION-29" title="(MGL-GD:@MGL-GD MGL-PAX:SECTION)">&#8593;</a> <a href="#x-28MGL-GD-3A-40MGL-GD-PER-WEIGHT-OPTIMIZATION-20MGL-PAX-3ASECTION-29" title="(MGL-GD:@MGL-GD-PER-WEIGHT-OPTIMIZATION MGL-PAX:SECTION)">&#8594;</a> <a href="#x-28MGL-GD-3A-40MGL-GD-SEGMENTED-GD-OPTIMIZER-20MGL-PAX-3ASECTION-29" title="(MGL-GD:@MGL-GD-SEGMENTED-GD-OPTIMIZER MGL-PAX:SECTION)">&#8634;</a></span>6.3.2 Segmented GD Optimizer</h4>

<p><a name='x-28MGL-GD-3ASEGMENTED-GD-OPTIMIZER-20CLASS-29'></a></p>

<ul>
<li><p>[class] <strong>SEGMENTED-GD-OPTIMIZER</strong> <em>BASE-GD-OPTIMIZER</em></p>

<p>An optimizer that delegates training of segments to
other optimizers. Useful to delegate training of different segments
to different optimizers (capable of working with segmentables) or
simply to not train all segments.</p></li>
</ul>

<p><a name='x-28MGL-OPT-3AN-INSTANCES-20-28MGL-PAX-3AREADER-20MGL-OPT-3AITERATIVE-OPTIMIZER-29-29'></a></p>

<ul>
<li><p>[reader] <strong>N-INSTANCES</strong> <em>ITERATIVE-OPTIMIZER</em> <em>(:N-INSTANCES = 0)</em></p>

<p>The number of instances this optimizer has seen so
far. Incremented automatically during optimization.</p></li>
</ul>

<p><a name='x-28MGL-OPT-3ATERMINATION-20-28MGL-PAX-3AACCESSOR-20MGL-OPT-3AITERATIVE-OPTIMIZER-29-29'></a></p>

<ul>
<li><p>[accessor] <strong>TERMINATION</strong> <em>ITERATIVE-OPTIMIZER</em> <em>(:TERMINATION = NIL)</em></p>

<p>If a number, it's the number of instances to train
on in the sense of <a href="#x-28MGL-OPT-3AN-INSTANCES-20-28MGL-PAX-3AREADER-20MGL-OPT-3AITERATIVE-OPTIMIZER-29-29" title="(MGL-OPT:N-INSTANCES (MGL-PAX:READER MGL-OPT:ITERATIVE-OPTIMIZER))"><code>N-INSTANCES</code></a>. If <a href="#x-28MGL-OPT-3AN-INSTANCES-20-28MGL-PAX-3AREADER-20MGL-OPT-3AITERATIVE-OPTIMIZER-29-29" title="(MGL-OPT:N-INSTANCES (MGL-PAX:READER MGL-OPT:ITERATIVE-OPTIMIZER))"><code>N-INSTANCES</code></a> is equal or greater
than this value optimization stops. If <a href="#x-28MGL-OPT-3ATERMINATION-20-28MGL-PAX-3AACCESSOR-20MGL-OPT-3AITERATIVE-OPTIMIZER-29-29" title="(MGL-OPT:TERMINATION (MGL-PAX:ACCESSOR MGL-OPT:ITERATIVE-OPTIMIZER))"><code>TERMINATION</code></a> is <code>NIL</code>, then
optimization will continue. If it is <code>T</code>, then optimization will
stop. If it is a function of no arguments, then its return value
is processed as if it was returned by <a href="#x-28MGL-OPT-3ATERMINATION-20-28MGL-PAX-3AACCESSOR-20MGL-OPT-3AITERATIVE-OPTIMIZER-29-29" title="(MGL-OPT:TERMINATION (MGL-PAX:ACCESSOR MGL-OPT:ITERATIVE-OPTIMIZER))"><code>TERMINATION</code></a>.</p></li>
</ul>

<p><a name='x-28MGL-GD-3ASEGMENTER-20-28MGL-PAX-3AREADER-20MGL-GD-3ASEGMENTED-GD-OPTIMIZER-29-29'></a></p>

<ul>
<li><p>[reader] <strong>SEGMENTER</strong> <em>SEGMENTED-GD-OPTIMIZER</em> <em>(:SEGMENTER)</em></p>

<p>When this optimizer is initialized it loops over
the segment of the learner with <a href="#x-28MGL-OPT-3AMAP-SEGMENTS-20GENERIC-FUNCTION-29" title="(MGL-OPT:MAP-SEGMENTS GENERIC-FUNCTION)"><code>MAP-SEGMENTS</code></a>. <a href="#x-28MGL-GD-3ASEGMENTER-20-28MGL-PAX-3AREADER-20MGL-GD-3ASEGMENTED-GD-OPTIMIZER-29-29" title="(MGL-GD:SEGMENTER (MGL-PAX:READER MGL-GD:SEGMENTED-GD-OPTIMIZER))"><code>SEGMENTER</code></a> is a
function that is called with each segment and returns an optimizer
or <code>NIL</code>. Several segments may be mapped to the same optimizer.
After the segment-&gt;optimizer mappings are collected, each
optimizer is initialized by INITIALIZE-OPTIMIZER with the list of
segments mapped to it.</p></li>
</ul>

<p><a name='x-28MGL-GD-3ASEGMENTS-20-28MGL-PAX-3AREADER-20MGL-GD-3ASEGMENTED-GD-OPTIMIZER-29-29'></a></p>

<ul>
<li>[reader] <strong>SEGMENTS</strong> <em>SEGMENTED-GD-OPTIMIZER</em></li>
</ul>

<p><a name='x-28MGL-GD-3A-40MGL-GD-PER-WEIGHT-OPTIMIZATION-20MGL-PAX-3ASECTION-29'></a></p>

<h4><span class="navigation"> <a href="#x-28MGL-GD-3A-40MGL-GD-SEGMENTED-GD-OPTIMIZER-20MGL-PAX-3ASECTION-29" title="(MGL-GD:@MGL-GD-SEGMENTED-GD-OPTIMIZER MGL-PAX:SECTION)">&#8592;</a> <a href="#x-28MGL-GD-3A-40MGL-GD-20MGL-PAX-3ASECTION-29" title="(MGL-GD:@MGL-GD MGL-PAX:SECTION)">&#8593;</a> <a href="#x-28MGL-CG-3A-40MGL-CG-20MGL-PAX-3ASECTION-29" title="(MGL-CG:@MGL-CG MGL-PAX:SECTION)">&#8594;</a> <a href="#x-28MGL-GD-3A-40MGL-GD-PER-WEIGHT-OPTIMIZATION-20MGL-PAX-3ASECTION-29" title="(MGL-GD:@MGL-GD-PER-WEIGHT-OPTIMIZATION MGL-PAX:SECTION)">&#8634;</a></span>6.3.3 Per-weight Optimization</h4>

<p><a name='x-28MGL-GD-3ANORMALIZED-BATCH-GD-OPTIMIZER-20CLASS-29'></a></p>

<ul>
<li><p>[class] <strong>NORMALIZED-BATCH-GD-OPTIMIZER</strong> <em>BATCH-GD-OPTIMIZER</em></p>

<p>Like <a href="#x-28MGL-GD-3ABATCH-GD-OPTIMIZER-20CLASS-29" title="(MGL-GD:BATCH-GD-OPTIMIZER CLASS)"><code>BATCH-GD-OPTIMIZER</code></a> but keeps count of how many
times each weight was used in the batch and divides the accumulated
gradient by this count instead of dividing by <code>N-INSTANCES-IN-BATCH</code>.
This only makes a difference if there are missing values in the
learner that's being trained. The main feature that distuinguishes
this class from <a href="#x-28MGL-GD-3APER-WEIGHT-BATCH-GD-OPTIMIZER-20CLASS-29" title="(MGL-GD:PER-WEIGHT-BATCH-GD-OPTIMIZER CLASS)"><code>PER-WEIGHT-BATCH-GD-OPTIMIZER</code></a> is that batches end at
same time for all weights.</p></li>
</ul>

<p><a name='x-28MGL-GD-3APER-WEIGHT-BATCH-GD-OPTIMIZER-20CLASS-29'></a></p>

<ul>
<li><p>[class] <strong>PER-WEIGHT-BATCH-GD-OPTIMIZER</strong> <em>GD-OPTIMIZER</em></p>

<p>This is much like <a href="#x-28MGL-GD-3ABATCH-GD-OPTIMIZER-20CLASS-29" title="(MGL-GD:BATCH-GD-OPTIMIZER CLASS)"><code>BATCH-GD-OPTIMIZER</code></a> but it is more
clever about when to update weights. Basically every weight has its
own batch independent from the batches of others. It has desirable
properties. One can for example put two neural networks together
without adding any connections between them and the learning will
produce results equivalent to the separated case. Also, adding
inputs with only missing values does not change anything.</p></li>
</ul>

<p><a name='x-28MGL-CG-3A-40MGL-CG-20MGL-PAX-3ASECTION-29'></a></p>

<h3><span class="navigation"> <a href="#x-28MGL-GD-3A-40MGL-GD-PER-WEIGHT-OPTIMIZATION-20MGL-PAX-3ASECTION-29" title="(MGL-GD:@MGL-GD-PER-WEIGHT-OPTIMIZATION MGL-PAX:SECTION)">&#8592;</a> <a href="#x-28MGL-OPT-3A-40MGL-OPT-20MGL-PAX-3ASECTION-29" title="(MGL-OPT:@MGL-OPT MGL-PAX:SECTION)">&#8593;</a> <a href="#x-28MGL-DIFFUN-3A-40MGL-DIFFUN-20MGL-PAX-3ASECTION-29" title="(MGL-DIFFUN:@MGL-DIFFUN MGL-PAX:SECTION)">&#8594;</a> <a href="#x-28MGL-CG-3A-40MGL-CG-20MGL-PAX-3ASECTION-29" title="(MGL-CG:@MGL-CG MGL-PAX:SECTION)">&#8634;</a></span>6.4 Conjugate Gradient</h3>

<h6>[in package MGL-CG]</h6>

<p>Conjugate gradient is a first-order optimization algorithm. It's
more advanced than gradient descent as it does line searches which
unfortunately also makes it unsuitable for non-deterministic
functions. Let's see how to minimize a numerical lisp function with
respect to some of its parameters.</p>

<pre><code><span class="code"><span class="comment">;;; Create an object representing the sine function.
</span><span class="paren1">(<span class="code"><i><span class="symbol">defparameter</span></i> <span class="special">*diff-fn-1*</span>
  <span class="paren2">(<span class="code">make-instance 'mgl-diffun:diffun
                 <span class="keyword">:fn</span> #'sin
                 <span class="comment">;; We are going to optimize its only parameter.
</span>                 <span class="keyword">:weight-indices</span> '<span class="paren3">(<span class="code">0</span>)</span></span>)</span></span>)</span>

<span class="comment">;;; Minimize SIN. Note that there is no dataset involved because all
</span><span class="comment">;;; parameters are being optimized.
</span><span class="paren1">(<span class="code">minimize <span class="paren2">(<span class="code">make-instance 'cg-optimizer
                         <span class="keyword">:batch-size</span> 1
                         <span class="keyword">:termination</span> 1</span>)</span>
          <span class="special">*diff-fn-1*</span>
          <span class="keyword">:weights</span> <span class="paren2">(<span class="code">make-mat 1</span>)</span></span>)</span>
<span class="comment">;;; =&gt; A MAT with a single value of about -pi/2.
</span>
<span class="comment">;;; Create a differentiable function for f(x,y)=(x-y)^2. X is a
</span><span class="comment">;;; parameter whose values come from the DATASET argument passed to
</span><span class="comment">;;; MINIMIZE. Y is a parameter to be optimized (a 'weight').
</span><span class="paren1">(<span class="code"><i><span class="symbol">defparameter</span></i> <span class="special">*diff-fn-2*</span>
  <span class="paren2">(<span class="code">make-instance 'mgl-diffun:diffun
                 <span class="keyword">:fn</span> <span class="paren3">(<span class="code"><i><span class="symbol">lambda</span></i> <span class="paren4">(<span class="code">x y</span>)</span>
                       <span class="paren4">(<span class="code">expt <span class="paren5">(<span class="code">- x y</span>)</span> 2</span>)</span></span>)</span>
                 <span class="keyword">:parameter-indices</span> '<span class="paren3">(<span class="code">0</span>)</span>
                 <span class="keyword">:weight-indices</span> '<span class="paren3">(<span class="code">1</span>)</span></span>)</span></span>)</span>

<span class="comment">;;; Find the Y that minimizes the distance from the instances
</span><span class="comment">;;; generated by the sampler.
</span><span class="paren1">(<span class="code">minimize <span class="paren2">(<span class="code">make-instance 'cg-optimizer <span class="keyword">:batch-size</span> 10</span>)</span>
          <span class="special">*diff-fn-2*</span>
          <span class="keyword">:weights</span> <span class="paren2">(<span class="code">make-mat 1</span>)</span>
          <span class="keyword">:dataset</span> <span class="paren2">(<span class="code">make-instance 'function-sampler
                                  <span class="keyword">:sampler</span> <span class="paren3">(<span class="code"><i><span class="symbol">lambda</span></i> <span class="paren4">(<span class="code"></span>)</span>
                                             <span class="paren4">(<span class="code">list <span class="paren5">(<span class="code">+ 10
                                                      <span class="paren6">(<span class="code">gaussian-random-1</span>)</span></span>)</span></span>)</span></span>)</span>
                                  <span class="keyword">:max-n-samples</span> 1000</span>)</span></span>)</span>
<span class="comment">;;; =&gt; A MAT with a single value of about 10, the expected value of
</span><span class="comment">;;; the instances in the dataset.
</span>
<span class="comment">;;; The dataset can be a SEQUENCE in which case we'd better set
</span><span class="comment">;;; TERMINATION else optimization would never finish. Note how a
</span><span class="comment">;;; single epoch suffices.
</span><span class="paren1">(<span class="code">minimize <span class="paren2">(<span class="code">make-instance 'cg-optimizer <span class="keyword">:termination</span> 6</span>)</span>
          <span class="special">*diff-fn-2*</span>
          <span class="keyword">:weights</span> <span class="paren2">(<span class="code">make-mat 1</span>)</span>
          <span class="keyword">:dataset</span> '<span class="paren2">(<span class="code"><span class="paren3">(<span class="code">0</span>)</span> <span class="paren3">(<span class="code">1</span>)</span> <span class="paren3">(<span class="code">2</span>)</span> <span class="paren3">(<span class="code">3</span>)</span> <span class="paren3">(<span class="code">4</span>)</span> <span class="paren3">(<span class="code">5</span>)</span></span>)</span></span>)</span>
<span class="comment">;;; =&gt; A MAT with a single value of about 2.5.</span></span></code></pre>

<p><a name='x-28MGL-CG-3ACG-20FUNCTION-29'></a></p>

<ul>
<li><p>[function] <strong>CG</strong> <em>FN W &amp;KEY (MAX-N-LINE-SEARCHES *DEFAULT-MAX-N-LINE-SEARCHES*) (MAX-N-EVALUATIONS-PER-LINE-SEARCH *DEFAULT-MAX-N-EVALUATIONS-PER-LINE-SEARCH*) (MAX-N-EVALUATIONS *DEFAULT-MAX-N-EVALUATIONS*) (SIG *DEFAULT-SIG*) (RHO *DEFAULT-RHO*) (INT *DEFAULT-INT*) (EXT *DEFAULT-EXT*) (RATIO *DEFAULT-RATIO*) SPARE-VECTORS</em></p>

<p><a href="#x-28MGL-CG-3ACG-OPTIMIZER-20CLASS-29" title="(MGL-CG:CG-OPTIMIZER CLASS)"><code>CG-OPTIMIZER</code></a> passes each batch of data to this function with its
<a href="#x-28MGL-CG-3ACG-ARGS-20-28MGL-PAX-3AACCESSOR-20MGL-CG-3ACG-OPTIMIZER-29-29" title="(MGL-CG:CG-ARGS (MGL-PAX:ACCESSOR MGL-CG:CG-OPTIMIZER))"><code>CG-ARGS</code></a> passed on.</p>

<p>Minimize a differentiable multivariate function with conjugate
gradient. The Polak-Ribiere flavour of conjugate gradients is used
to compute search directions, and a line search using quadratic and
cubic polynomial approximations and the Wolfe-Powell stopping
criteria is used together with the slope ratio method for guessing
initial step sizes. Additionally a bunch of checks are made to make
sure that exploration is taking place and that extrapolation will
not be unboundedly large.</p>

<p><code>FN</code> is a function of two parameters: <code>WEIGHTS</code> and <code>DERIVATIVES</code>. <code>WEIGHTS</code>
is a <code>MAT</code> of the same size as <code>W</code> that is where the search start from.
<code>DERIVATIVES</code> is also a <code>MAT</code> of that size and it is where <code>FN</code> shall
place the partial derivatives. <code>FN</code> returns the value of the function
that is being minimized.</p>

<p><a href="#x-28MGL-CG-3ACG-20FUNCTION-29" title="(MGL-CG:CG FUNCTION)"><code>CG</code></a> performs a number of line searches and invokes <code>FN</code> at each step. A
line search invokes <code>FN</code> at most <code>MAX-N-EVALUATIONS-PER-LINE-SEARCH</code>
number of times and can succeed in improving the minimum by the
sufficient margin or it can fail. Note, the even a failed line
search may improve further and hence change the weights it's just
that the improvement was deemed too small. <a href="#x-28MGL-CG-3ACG-20FUNCTION-29" title="(MGL-CG:CG FUNCTION)"><code>CG</code></a> stops when either:</p>

<ul>
<li><p>two line searches fail in a row</p></li>
<li><p><code>MAX-N-LINE-SEARCHES</code> is reached</p></li>
<li><p><code>MAX-N-EVALUATIONS</code> is reached</p></li>
</ul>

<p><a href="#x-28MGL-CG-3ACG-20FUNCTION-29" title="(MGL-CG:CG FUNCTION)"><code>CG</code></a> returns a <code>MAT</code> that contains the best weights, the minimum, the
number of line searches performed, the number of succesful line
searches and the number of evaluations.</p>

<p>When using <code>MAX-N-EVALUATIONS</code> remember that there is an extra
evaluation of <code>FN</code> before the first line search.</p>

<p><code>SPARE-VECTORS</code> is a list of preallocated MATs of the same size as <code>W</code>.
Passing 6 of them covers the current need of the algorithm and it
will not cons up vectors of size <code>W</code> at all.</p>

<p>NOTE: If the function terminates within a few iterations, it could
be an indication that the function values and derivatives are not
consistent (ie, there may be a bug in the implementation of <code>FN</code>
function).</p>

<p><code>SIG</code> and <code>RHO</code> are the constants controlling the Wolfe-Powell
conditions. <code>SIG</code> is the maximum allowed absolute ratio between
previous and new slopes (derivatives in the search direction), thus
setting <code>SIG</code> to low (positive) values forces higher precision in the
line-searches. <code>RHO</code> is the minimum allowed fraction of the
expected (from the slope at the initial point in the linesearch).
Constants must satisfy 0 &lt; <code>RHO</code> &lt; <code>SIG</code> &lt; 1. Tuning of <code>SIG</code> (depending
on the nature of the function to be optimized) may speed up the
minimization; it is probably not worth playing much with <code>RHO</code>.</p></li>
</ul>

<p><a name='x-28MGL-CG-3A-2ADEFAULT-INT-2A-20VARIABLE-29'></a></p>

<ul>
<li><p>[variable] <strong>*DEFAULT-INT*</strong> <em>0.1</em></p>

<p>Don't reevaluate within <code>INT</code> of the limit of the current bracket.</p></li>
</ul>

<p><a name='x-28MGL-CG-3A-2ADEFAULT-EXT-2A-20VARIABLE-29'></a></p>

<ul>
<li><p>[variable] <strong>*DEFAULT-EXT*</strong> <em>3</em></p>

<p>Extrapolate maximum <code>EXT</code> times the current step-size.</p></li>
</ul>

<p><a name='x-28MGL-CG-3A-2ADEFAULT-SIG-2A-20VARIABLE-29'></a></p>

<ul>
<li><p>[variable] <strong>*DEFAULT-SIG*</strong> <em>0.1</em></p>

<p><code>SIG</code> and <code>RHO</code> are the constants controlling the Wolfe-Powell
conditions. <code>SIG</code> is the maximum allowed absolute ratio between
previous and new slopes (derivatives in the search direction), thus
setting <code>SIG</code> to low (positive) values forces higher precision in the
line-searches.</p></li>
</ul>

<p><a name='x-28MGL-CG-3A-2ADEFAULT-RHO-2A-20VARIABLE-29'></a></p>

<ul>
<li><p>[variable] <strong>*DEFAULT-RHO*</strong> <em>0.05</em></p>

<p><code>RHO</code> is the minimum allowed fraction of the expected (from the slope
at the initial point in the linesearch). Constants must satisfy 0 &lt;
<code>RHO</code> &lt; <code>SIG</code> &lt; 1.</p></li>
</ul>

<p><a name='x-28MGL-CG-3A-2ADEFAULT-RATIO-2A-20VARIABLE-29'></a></p>

<ul>
<li><p>[variable] <strong>*DEFAULT-RATIO*</strong> <em>10</em></p>

<p>Maximum allowed slope ratio.</p></li>
</ul>

<p><a name='x-28MGL-CG-3A-2ADEFAULT-MAX-N-LINE-SEARCHES-2A-20VARIABLE-29'></a></p>

<ul>
<li>[variable] <strong>*DEFAULT-MAX-N-LINE-SEARCHES*</strong> <em>NIL</em></li>
</ul>

<p><a name='x-28MGL-CG-3A-2ADEFAULT-MAX-N-EVALUATIONS-PER-LINE-SEARCH-2A-20VARIABLE-29'></a></p>

<ul>
<li>[variable] <strong>*DEFAULT-MAX-N-EVALUATIONS-PER-LINE-SEARCH*</strong> <em>20</em></li>
</ul>

<p><a name='x-28MGL-CG-3A-2ADEFAULT-MAX-N-EVALUATIONS-2A-20VARIABLE-29'></a></p>

<ul>
<li>[variable] <strong>*DEFAULT-MAX-N-EVALUATIONS*</strong> <em>NIL</em></li>
</ul>

<p><a name='x-28MGL-CG-3ACG-OPTIMIZER-20CLASS-29'></a></p>

<ul>
<li><p>[class] <strong>CG-OPTIMIZER</strong> <em>ITERATIVE-OPTIMIZER</em></p>

<p>Updates all weights simultaneously after chewing
through <code>BATCH-SIZE</code>(<a href="#x-28MGL-COMMON-3ABATCH-SIZE-20-28MGL-PAX-3AACCESSOR-20MGL-CG-3ACG-OPTIMIZER-29-29" title="(MGL-COMMON:BATCH-SIZE (MGL-PAX:ACCESSOR MGL-CG:CG-OPTIMIZER))"><code>0</code></a> <a href="#x-28MGL-COMMON-3ABATCH-SIZE-20-28MGL-PAX-3AACCESSOR-20MGL-GD-3A-3AGD-OPTIMIZER-29-29" title="(MGL-COMMON:BATCH-SIZE (MGL-PAX:ACCESSOR MGL-GD::GD-OPTIMIZER))"><code>1</code></a>) inputs.</p></li>
</ul>

<p><a name='x-28MGL-OPT-3AN-INSTANCES-20-28MGL-PAX-3AREADER-20MGL-OPT-3AITERATIVE-OPTIMIZER-29-29'></a></p>

<ul>
<li><p>[reader] <strong>N-INSTANCES</strong> <em>ITERATIVE-OPTIMIZER</em> <em>(:N-INSTANCES = 0)</em></p>

<p>The number of instances this optimizer has seen so
far. Incremented automatically during optimization.</p></li>
</ul>

<p><a name='x-28MGL-OPT-3ATERMINATION-20-28MGL-PAX-3AACCESSOR-20MGL-OPT-3AITERATIVE-OPTIMIZER-29-29'></a></p>

<ul>
<li><p>[accessor] <strong>TERMINATION</strong> <em>ITERATIVE-OPTIMIZER</em> <em>(:TERMINATION = NIL)</em></p>

<p>If a number, it's the number of instances to train
on in the sense of <a href="#x-28MGL-OPT-3AN-INSTANCES-20-28MGL-PAX-3AREADER-20MGL-OPT-3AITERATIVE-OPTIMIZER-29-29" title="(MGL-OPT:N-INSTANCES (MGL-PAX:READER MGL-OPT:ITERATIVE-OPTIMIZER))"><code>N-INSTANCES</code></a>. If <a href="#x-28MGL-OPT-3AN-INSTANCES-20-28MGL-PAX-3AREADER-20MGL-OPT-3AITERATIVE-OPTIMIZER-29-29" title="(MGL-OPT:N-INSTANCES (MGL-PAX:READER MGL-OPT:ITERATIVE-OPTIMIZER))"><code>N-INSTANCES</code></a> is equal or greater
than this value optimization stops. If <a href="#x-28MGL-OPT-3ATERMINATION-20-28MGL-PAX-3AACCESSOR-20MGL-OPT-3AITERATIVE-OPTIMIZER-29-29" title="(MGL-OPT:TERMINATION (MGL-PAX:ACCESSOR MGL-OPT:ITERATIVE-OPTIMIZER))"><code>TERMINATION</code></a> is <code>NIL</code>, then
optimization will continue. If it is <code>T</code>, then optimization will
stop. If it is a function of no arguments, then its return value
is processed as if it was returned by <a href="#x-28MGL-OPT-3ATERMINATION-20-28MGL-PAX-3AACCESSOR-20MGL-OPT-3AITERATIVE-OPTIMIZER-29-29" title="(MGL-OPT:TERMINATION (MGL-PAX:ACCESSOR MGL-OPT:ITERATIVE-OPTIMIZER))"><code>TERMINATION</code></a>.</p></li>
</ul>

<p><a name='x-28MGL-COMMON-3ABATCH-SIZE-20-28MGL-PAX-3AACCESSOR-20MGL-CG-3ACG-OPTIMIZER-29-29'></a></p>

<ul>
<li><p>[accessor] <strong>BATCH-SIZE</strong> <em>CG-OPTIMIZER</em> <em>(:BATCH-SIZE)</em></p>

<p>After having gone through <code>BATCH-SIZE</code>(<a href="#x-28MGL-COMMON-3ABATCH-SIZE-20-28MGL-PAX-3AACCESSOR-20MGL-CG-3ACG-OPTIMIZER-29-29" title="(MGL-COMMON:BATCH-SIZE (MGL-PAX:ACCESSOR MGL-CG:CG-OPTIMIZER))"><code>0</code></a> <a href="#x-28MGL-COMMON-3ABATCH-SIZE-20-28MGL-PAX-3AACCESSOR-20MGL-GD-3A-3AGD-OPTIMIZER-29-29" title="(MGL-COMMON:BATCH-SIZE (MGL-PAX:ACCESSOR MGL-GD::GD-OPTIMIZER))"><code>1</code></a>) number of
instances, weights are updated. Normally, <a href="#x-28MGL-CG-3ACG-20FUNCTION-29" title="(MGL-CG:CG FUNCTION)"><code>CG</code></a> operates on all
available data, but it may be useful to introduce some noise into
the optimization to reduce overfitting by using smaller batch
sizes. If <code>BATCH-SIZE</code>(<a href="#x-28MGL-COMMON-3ABATCH-SIZE-20-28MGL-PAX-3AACCESSOR-20MGL-CG-3ACG-OPTIMIZER-29-29" title="(MGL-COMMON:BATCH-SIZE (MGL-PAX:ACCESSOR MGL-CG:CG-OPTIMIZER))"><code>0</code></a> <a href="#x-28MGL-COMMON-3ABATCH-SIZE-20-28MGL-PAX-3AACCESSOR-20MGL-GD-3A-3AGD-OPTIMIZER-29-29" title="(MGL-COMMON:BATCH-SIZE (MGL-PAX:ACCESSOR MGL-GD::GD-OPTIMIZER))"><code>1</code></a>) is not set, it is initialized to the size of
the dataset at the start of optimization.</p></li>
</ul>

<p><a name='x-28MGL-CG-3ACG-ARGS-20-28MGL-PAX-3AACCESSOR-20MGL-CG-3ACG-OPTIMIZER-29-29'></a></p>

<ul>
<li>[accessor] <strong>CG-ARGS</strong> <em>CG-OPTIMIZER</em> <em>(:CG-ARGS = 'NIL)</em></li>
</ul>

<p><a name='x-28MGL-CG-3ASEGMENT-FILTER-20-28MGL-PAX-3AREADER-20MGL-CG-3ACG-OPTIMIZER-29-29'></a></p>

<ul>
<li><p>[reader] <strong>SEGMENT-FILTER</strong> <em>CG-OPTIMIZER</em> <em>(:SEGMENT-FILTER = (CONSTANTLY T))</em></p>

<p>A predicate function on segments that filters out
uninteresting segments. Called from <a href="#x-28MGL-OPT-3AINITIALIZE-OPTIMIZER-2A-20GENERIC-FUNCTION-29" title="(MGL-OPT:INITIALIZE-OPTIMIZER* GENERIC-FUNCTION)"><code>INITIALIZE-OPTIMIZER*</code></a>.</p></li>
</ul>

<p><a name='x-28MGL-CG-3ADECAYED-CG-OPTIMIZER-MIXIN-20CLASS-29'></a></p>

<ul>
<li><p>[class] <strong>DECAYED-CG-OPTIMIZER-MIXIN</strong></p>

<p>Mix this before a <a href="#x-28MGL-CG-3ACG-20FUNCTION-29" title="(MGL-CG:CG FUNCTION)"><code>CG</code></a> based optimizer to conveniently
add decay on a per-segment basis.</p></li>
</ul>

<p><a name='x-28MGL-CG-3ASEGMENT-DECAY-FN-20-28MGL-PAX-3AACCESSOR-20MGL-CG-3ADECAYED-CG-OPTIMIZER-MIXIN-29-29'></a></p>

<ul>
<li><p>[accessor] <strong>SEGMENT-DECAY-FN</strong> <em>DECAYED-CG-OPTIMIZER-MIXIN</em> <em>(:SEGMENT-DECAY-FN = NIL)</em></p>

<p>If not <code>NIL</code>, it's a designator for a function that
returns the decay for a given segment. For convenience <code>NIL</code> is also
treated as 0 decay.</p></li>
</ul>

<p><a name='x-28MGL-DIFFUN-3A-40MGL-DIFFUN-20MGL-PAX-3ASECTION-29'></a></p>

<h2><span class="navigation"> <a href="#x-28MGL-CG-3A-40MGL-CG-20MGL-PAX-3ASECTION-29" title="(MGL-CG:@MGL-CG MGL-PAX:SECTION)">&#8592;</a> <a href="#x-28MGL-3A-40MGL-MANUAL-20MGL-PAX-3ASECTION-29" title="(MGL:@MGL-MANUAL MGL-PAX:SECTION)">&#8593;</a> <a href="#x-28MGL-3A-40MGL-BP-20MGL-PAX-3ASECTION-29" title="(MGL:@MGL-BP MGL-PAX:SECTION)">&#8594;</a> <a href="#x-28MGL-DIFFUN-3A-40MGL-DIFFUN-20MGL-PAX-3ASECTION-29" title="(MGL-DIFFUN:@MGL-DIFFUN MGL-PAX:SECTION)">&#8634;</a></span>7 Differentiable Function</h2>

<h6>[in package MGL-DIFFUN]</h6>

<p><a name='x-28MGL-DIFFUN-3ADIFFUN-20CLASS-29'></a></p>

<ul>
<li><p>[class] <strong>DIFFUN</strong></p>

<p><a href="#x-28MGL-DIFFUN-3ADIFFUN-20CLASS-29" title="(MGL-DIFFUN:DIFFUN CLASS)"><code>DIFFUN</code></a> dresses a lisp function (in its <a href="#x-28MGL-DIFFUN-3AFN-20-28MGL-PAX-3AREADER-20MGL-DIFFUN-3ADIFFUN-29-29" title="(MGL-DIFFUN:FN (MGL-PAX:READER MGL-DIFFUN:DIFFUN))"><code>FN</code></a> slot) as
a gradient source (see <a href="#x-28MGL-OPT-3A-40MGL-OPT-GRADIENT-SOURCE-20MGL-PAX-3ASECTION-29" title="(MGL-OPT:@MGL-OPT-GRADIENT-SOURCE MGL-PAX:SECTION)">Implementing Gradient Sources</a>) which allows it to
be used in <a href="#x-28MGL-OPT-3AMINIMIZE-20FUNCTION-29" title="(MGL-OPT:MINIMIZE FUNCTION)"><code>MINIMIZE</code></a>. See the examples in <a href="#x-28MGL-GD-3A-40MGL-GD-20MGL-PAX-3ASECTION-29" title="(MGL-GD:@MGL-GD MGL-PAX:SECTION)">Gradient Descent</a> and
<a href="#x-28MGL-CG-3A-40MGL-CG-20MGL-PAX-3ASECTION-29" title="(MGL-CG:@MGL-CG MGL-PAX:SECTION)">Conjugate Gradient</a>.</p></li>
</ul>

<p><a name='x-28MGL-DIFFUN-3AFN-20-28MGL-PAX-3AREADER-20MGL-DIFFUN-3ADIFFUN-29-29'></a></p>

<ul>
<li><p>[reader] <strong>FN</strong> <em>DIFFUN</em> <em>(:FN)</em></p>

<p>A lisp function. It may have any number of
parameters.</p></li>
</ul>

<p><a name='x-28MGL-DIFFUN-3APARAMETER-INDICES-20-28MGL-PAX-3AREADER-20MGL-DIFFUN-3ADIFFUN-29-29'></a></p>

<ul>
<li><p>[reader] <strong>PARAMETER-INDICES</strong> <em>DIFFUN</em> <em>(:PARAMETER-INDICES = NIL)</em></p>

<p>The list of indices of parameters that we don't
optimize. Values for these will come from the DATASET argument of
<a href="#x-28MGL-OPT-3AMINIMIZE-20FUNCTION-29" title="(MGL-OPT:MINIMIZE FUNCTION)"><code>MINIMIZE</code></a>.</p></li>
</ul>

<p><a name='x-28MGL-DIFFUN-3AWEIGHT-INDICES-20-28MGL-PAX-3AREADER-20MGL-DIFFUN-3ADIFFUN-29-29'></a></p>

<ul>
<li><p>[reader] <strong>WEIGHT-INDICES</strong> <em>DIFFUN</em> <em>(:WEIGHT-INDICES = NIL)</em></p>

<p>The list of indices of parameters to be optimized,
the values of which will come from the <code>WEIGHTS</code> argument of
<a href="#x-28MGL-OPT-3AMINIMIZE-20FUNCTION-29" title="(MGL-OPT:MINIMIZE FUNCTION)"><code>MINIMIZE</code></a>.</p></li>
</ul>

<p><a name='x-28MGL-3A-40MGL-BP-20MGL-PAX-3ASECTION-29'></a></p>

<h2><span class="navigation"> <a href="#x-28MGL-DIFFUN-3A-40MGL-DIFFUN-20MGL-PAX-3ASECTION-29" title="(MGL-DIFFUN:@MGL-DIFFUN MGL-PAX:SECTION)">&#8592;</a> <a href="#x-28MGL-3A-40MGL-MANUAL-20MGL-PAX-3ASECTION-29" title="(MGL:@MGL-MANUAL MGL-PAX:SECTION)">&#8593;</a> <a href="#x-28MGL-3A-40MGL-BM-20MGL-PAX-3ASECTION-29" title="(MGL:@MGL-BM MGL-PAX:SECTION)">&#8594;</a> <a href="#x-28MGL-3A-40MGL-BP-20MGL-PAX-3ASECTION-29" title="(MGL:@MGL-BP MGL-PAX:SECTION)">&#8634;</a></span>8 Backprogation Neural Networks</h2>

<p><a name='x-28MGL-3A-40MGL-BM-20MGL-PAX-3ASECTION-29'></a></p>

<h2><span class="navigation"> <a href="#x-28MGL-3A-40MGL-BP-20MGL-PAX-3ASECTION-29" title="(MGL:@MGL-BP MGL-PAX:SECTION)">&#8592;</a> <a href="#x-28MGL-3A-40MGL-MANUAL-20MGL-PAX-3ASECTION-29" title="(MGL:@MGL-MANUAL MGL-PAX:SECTION)">&#8593;</a> <a href="#x-28MGL-3A-40MGL-GP-20MGL-PAX-3ASECTION-29" title="(MGL:@MGL-GP MGL-PAX:SECTION)">&#8594;</a> <a href="#x-28MGL-3A-40MGL-BM-20MGL-PAX-3ASECTION-29" title="(MGL:@MGL-BM MGL-PAX:SECTION)">&#8634;</a></span>9 Boltzmann Machines</h2>

<p><a name='x-28MGL-3A-40MGL-GP-20MGL-PAX-3ASECTION-29'></a></p>

<h2><span class="navigation"> <a href="#x-28MGL-3A-40MGL-BM-20MGL-PAX-3ASECTION-29" title="(MGL:@MGL-BM MGL-PAX:SECTION)">&#8592;</a> <a href="#x-28MGL-3A-40MGL-MANUAL-20MGL-PAX-3ASECTION-29" title="(MGL:@MGL-MANUAL MGL-PAX:SECTION)">&#8593;</a> <a href="#x-28MGL-3A-40MGL-GP-20MGL-PAX-3ASECTION-29" title="(MGL:@MGL-GP MGL-PAX:SECTION)">&#8634;</a></span>10 Gaussian Processes</h2>
<hr/><h6>[generated by
          <a href="https://github.com/melisgl/mgl-pax">MGL-PAX</a>]
          </h6>
          </body>
</html>
