MGL

Common Lisp machine learning library by [[http://quotenil.com][GÃ¡bor Melis]] with some parts
contributed by Ravenpack International. It implements:
- Backpropagation networks (BPN)
  - Dropout
  - Maxout
- Boltzmann Machines
  - Restricted Boltzmann Machines (RBM)
  - Deep Belief Networks (DBN)
  - Semi Restricted Boltzmann Machines
  - Boltzmann Machines
  - Unrolling DBN to a BPN
  - Contrastive Divergence (CD) learning
  - Persistent Contrastive Divergence (PCD) learning
- Gradient descent optimization
- Conjugate gradient optimization
- Gaussian Processes
  - Optimizing Gaussian Processes as BPNs

MGL and the bundled libraries are under the MIT licence. See COPYING.

* Bundled software

With MGL-PAX and MGL-MAT libraries split off there remains only a
single library bundled with MGL which do not depend on the rest of
MGL:

- MGL-GNUPLOT, a plotting library.

There is also MGL-VISUALS which does depend on MGL.

* Features

In general, the focus is on power and performance not on ease of use.
For example, it's possible to:
- control the order of presentation of training examples,
- vary learning rate depending on time, state of the trainer object,
- track all kinds of statistics during training,
etc.

Perhaps one day there will be a cookie cutter interface with
restricted functionality if a reasonable compromise is found between
power and utility.

* Tests

Run the built in tests with:

  (ASDF:OOS 'ASDF:TEST-OP '#:MGL)

Note, that most of the tests are rather stochastic and can fail once
in a while.

* BLAS

MGL uses on [[https://github.com/tpapp/lla][LLA]] to interface to BLAS and LAPACK. See the README in LLA
on how to set things up. Note that these days OpenBLAS is easier to
set up and just as fast as ATLAS.

* CUDA

CL-CUDA is a dependency for which the NVIDIA CUDA Toolkit needs to be
installed, but MGL is fully functional even if there is no cuda
capable gpu installed. See the MGL-MAT:WITH-CUDA macro for how to use
it.
